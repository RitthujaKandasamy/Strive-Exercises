{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import spacy \n",
    "from torchtext.vocab import FastText\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\15. Toxicity detect\\train.csv\\train.csv')\n",
    "\n",
    "data = data.drop('id', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_engineering(data):\n",
    "     column_name, no, yes = [], [], []\n",
    "     for i in data.columns:\n",
    "          if i == 'comment_text':\n",
    "               continue\n",
    "          else:\n",
    "               column_name.append(i),   no.append(data[i].value_counts()[0]),   yes.append(data[i].value_counts()[1])\n",
    "     return column_name, no, yes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: text.lower()) \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: text.strip()) \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\\n', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub(',', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('.', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\\'', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\"', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('! ', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('/', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('-', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\\w*\\d\\w*\\*', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub(r'[^\\x00-\\x7f]', r' ', text)) \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: text.strip())\n",
    "     return data \n",
    "\n",
    "data = preprocessing(data)\n",
    "\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "     data_toxic          = data[['comment_text', 'toxic']]\n",
    "     data_severe_toxic   = data[['comment_text', 'severe_toxic']]\n",
    "     data_obscene        = data[['comment_text', 'obscene']]\n",
    "     data_threat         = data[['comment_text', 'threat']]\n",
    "     data_insult         = data[['comment_text', 'insult']]\n",
    "     data_identity_hate  = data[['comment_text', 'identity_hate']]\n",
    "     return data_toxic, data_severe_toxic, data_obscene, data_threat, data_insult, data_identity_hate \n",
    "\n",
    "\n",
    "data_toxic, data_severe_toxic, data_obscene, data_threat, data_insult, data_identity_hate = split_data(data) \n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# split\n",
    "def train_test_split(df, train_size=0.7):\n",
    "\n",
    "    df_idx = [i for i in range(len(df))]\n",
    "    np.random.shuffle(df_idx)\n",
    "\n",
    "    len_train = int(len(df) * train_size)\n",
    "    train_idx, test_idx = df_idx[:len_train], df_idx[len_train:]\n",
    "\n",
    "    return df.iloc[train_idx].reset_index(drop=True), df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def token_encoder(token, vec):\n",
    "    if token == \"<pad>\":\n",
    "        return 1\n",
    "    else:\n",
    "        try:\n",
    "            return vec.stoi[token]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "def encoder(tokens, vec):\n",
    "    return [token_encoder(token, vec) for token in tokens]\n",
    "\n",
    "\n",
    "def front_padding(list_of_indexes, max_seq_len, padding_index=1):\n",
    "    new_out = (max_seq_len - len(list_of_indexes))*[padding_index] + list_of_indexes\n",
    "    return new_out[:max_seq_len]  \n",
    "\n",
    "\n",
    "\n",
    "fasttext = FastText(\"simple\")\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, data,data_target, max_seq_len=32): # data is the input data, max_seq_len is the max lenght allowed to a sentence before cutting or padding\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        \n",
    "        train_iter = iter(data['comment_text'].values)\n",
    "        self.vec = FastText(\"simple\")\n",
    "\n",
    "        self.vec.vectors[1] = -torch.ones(self.vec.vectors[1].shape[0]) # replacing the vector associated with 1 (padded value) to become a vector of -1.\n",
    "        self.vec.vectors[0] = torch.zeros(self.vec.vectors[0].shape[0]) # replacing the vector associated with 0 (unknown) to become zeros\n",
    "        self.vectorizer = lambda x: self.vec.vectors[x]\n",
    "\n",
    "        self.target = data[data_target]\n",
    "        features = [front_padding(encoder(preprocessing(sequence), self.vec), max_seq_len) for sequence in data['comment_text'].tolist()]\n",
    "        self.features = features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        assert len(self.features[i]) == self.max_seq_len\n",
    "        return self.features[i], self.target[i]\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(data_toxic)\n",
    "\n",
    "train_dataset = Data(train_df, data_target = 'toxic')\n",
    "test_dataset = Data(test_df, data_target = 'toxic')\n",
    "\n",
    "\n",
    "def traincollate(batch, vectorizer= train_dataset.vectorizer):\n",
    "    inputs = torch.stack([torch.stack([vectorizer(token) for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch]) \n",
    "    return inputs, target\n",
    "\n",
    "\n",
    "def testcollate(batch, vectorizer = test_dataset.vectorizer):\n",
    "    inputs = torch.stack([torch.stack([vectorizer(token) for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch]) \n",
    "    return inputs, target\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn= traincollate, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn= testcollate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim, hidden1=16, hidden2=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(max_seq_len*emb_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.out = nn.Sigmoid\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1).float()))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(32, 300)\n",
    "\n",
    "\n",
    "\n",
    "# loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.003)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\15. Toxicity detect\\check.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000004?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000004?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000004?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(sentences) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000004?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000004?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()                  \n",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\15. Toxicity detect\\check.ipynb Cell 3'\u001b[0m in \u001b[0;36mClassifier.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000002?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000002?line=12'>13</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/15.%20Toxicity%20detect/check.ipynb#ch0000002?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout(x)\n",
      "\u001b[1;31mTypeError\u001b[0m: Module.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "print_every = 40\n",
    "emb_dim = 300\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for i, (sentences, labels) in enumerate(iter(train_loader)):\n",
    "\n",
    "        sentences.resize_(sentences.size()[0], 32* emb_dim)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(sentences) \n",
    "        loss = criterion(output, labels) \n",
    "        loss.backward()                  \n",
    "        optimizer.step()                 \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loss = running_loss/ len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_running_loss = 0\n",
    "            \n",
    "            for i, (test_sentences, test_labels) in enumerate(iter(test_loader)):\n",
    "                test_sentences.resize_(test_sentences.size()[0], 32* emb_dim)\n",
    "\n",
    "                test_out = model.forward(test_sentences)\n",
    "                testloss = criterion(test_out, test_labels)\n",
    "                test_running_loss += testloss.item()\n",
    "                test_loss = test_running_loss / len(test_loader)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(train_loss, label= \"Train Loss\")\n",
    "plt.plot(test_loss, label= \"Test Loss\")\n",
    "plt.xlabel(\" Iteration \")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
