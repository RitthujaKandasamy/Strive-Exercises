{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import spacy \n",
    "from torchtext.vocab import FastText\n",
    "from collections import Counter\n",
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "#from model import ToxicClassifier \n",
    "\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\ritth\\code\\Strive\\toxic-detection-challenge\\train.csv\\train.csv')\n",
    "# print(data.head(10))\n",
    "# print(data.columns)\n",
    "data = data.drop('id', axis=1)\n",
    "# print(data)\n",
    "\n",
    "def data_engineering(data):\n",
    "     column_name, no, yes = [], [], []\n",
    "     for i in data.columns:\n",
    "          if i == 'comment_text':\n",
    "               continue\n",
    "          else:\n",
    "               column_name.append(i),   no.append(data[i].value_counts()[0]),   yes.append(data[i].value_counts()[1])\n",
    "     return column_name, no, yes\n",
    "\n",
    "# column_name, no, yes = data_engineering(data)\n",
    "\n",
    "# print(column_name)\n",
    "# print(no)\n",
    "# print(yes)\n",
    "\n",
    "# print('BEFORE')\n",
    "# print(data['comment_text'][3])\n",
    "\n",
    "def preprocessing(data):\n",
    "     # data = data \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: text.lower()) \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: text.strip()) \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\\n', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub(',', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('.', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\\'', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\"', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('! ', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('/', '', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('-', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('=', ' ', text))\n",
    "     # data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('(', ' ', text))\n",
    "     # data['comment_text'] = data['comment_text'].apply(lambda text: re.sub(')', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub('\\w*\\d\\w*\\*', ' ', text))\n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: re.sub(r'[^\\x00-\\x7f]', r' ', text)) \n",
    "     data['comment_text'] = data['comment_text'].apply(lambda text: text.strip())\n",
    "     return data \n",
    "\n",
    "# print()\n",
    "# print('AFTER')\n",
    "data = (preprocessing(data))\n",
    "# print(data['comment_text'][3])\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# print(data.head())\n",
    "\n",
    "def shuffle_data(dataset):\n",
    "     return dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = shuffle_data(data)\n",
    "# print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "     data_toxic          = data[['comment_text', 'toxic']]\n",
    "     data_severe_toxic   = data[['comment_text', 'severe_toxic']]\n",
    "     data_obscene        = data[['comment_text', 'obscene']]\n",
    "     data_threat         = data[['comment_text', 'threat']]\n",
    "     data_insult         = data[['comment_text', 'insult']]\n",
    "     data_identity_hate  = data[['comment_text', 'identity_hate']]\n",
    "     return data_toxic, data_severe_toxic, data_obscene, data_threat, data_insult, data_identity_hate \n",
    "\n",
    "\n",
    "# data_toxic, data_severe_toxic, data_obscene, data_threat, data_insult, data_identity_hate = split_data(data) \n",
    "\n",
    "# print()\n",
    "# print(data_toxic.head(5))\n",
    "# print()\n",
    "# print(data_severe_toxic.head(5))\n",
    "# print()\n",
    "# print(data_obscene.head(5))\n",
    "# print()\n",
    "# print(data_threat.head(5))\n",
    "# print()\n",
    "# print(data_insult.head(5))\n",
    "# print()\n",
    "# print(data_identity_hate.head(5))\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_toxic_few['comment_text'] = data_toxic_few['comment_text'].apply(lambda text: preprocessing(text))\n",
    "# print(data_toxic_few)\n",
    "\n",
    "\n",
    "\n",
    "def token_encoder(token, vec):\n",
    "    if token == \"<pad>\":\n",
    "        return 1\n",
    "    else:\n",
    "        try:\n",
    "            return vec.stoi[token]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "def encoder(tokens, vec):\n",
    "    return [token_encoder(token, vec) for token in tokens]\n",
    "\n",
    "\n",
    "def front_padding(list_of_indexes, max_seq_len, padding_index=0):\n",
    "    new_out = (max_seq_len - len(list_of_indexes))*[padding_index] + list_of_indexes\n",
    "    return new_out[:max_seq_len]  \n",
    "\n",
    "\n",
    "fasttext = FastText(\"simple\")\n",
    "\n",
    "\n",
    "max_seq_length = 50\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, data, data_target, max_seq_len=max_seq_length): # data is the input data, max_seq_len is the max lenght allowed to a sentence before cutting or padding\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        counter = Counter()\n",
    "        train_iter = iter(data['comment_text'].values)\n",
    "        self.vec = FastText(\"simple\")\n",
    "\n",
    "        self.vec.vectors[1] = -torch.ones(self.vec.vectors[1].shape[0]) # replacing the vector associated with 1 (padded value) to become a vector of -1.\n",
    "        self.vec.vectors[0] = torch.zeros(self.vec.vectors[0].shape[0]) # replacing the vector associated with 0 (unknown) to become zeros\n",
    "        self.vectorizer = lambda x: self.vec.vectors[x]\n",
    "\n",
    "        self.target = data[data_target]\n",
    "        features = [front_padding(encoder(preprocessing(sequence), self.vec), max_seq_len) for sequence in data['comment_text'].tolist()]\n",
    "        self.features = features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        assert len(self.features[i]) == self.max_seq_len\n",
    "        return self.features[i], self.target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "emb_dim = 300\n",
    "\n",
    "class ToxicClassifier(nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim, hidden=32):\n",
    "        super(ToxicClassifier, self).__init__()\n",
    "        self.input_layer   = nn.Linear(max_seq_len*emb_dim, hidden)\n",
    "        self.first_hidden  = nn.Linear(hidden, hidden)\n",
    "        self.second_hidden = nn.Linear(hidden, hidden)\n",
    "        self.third_hidden  = nn.Linear(hidden, 2)\n",
    "        self.output        = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.input_layer(inputs.squeeze(1).float()))\n",
    "        x = self.dropout(F.relu(self.first_hidden(x)))\n",
    "        x = self.dropout(F.relu(self.second_hidden(x)))\n",
    "        x = self.third_hidden(x)\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from data_handler import split_data, TrainData, data \n",
    "from model import ToxicClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "length_of_data = 159571 \n",
    "max_seq_length = 64 \n",
    "idx = int(0.7 * length_of_data)\n",
    "\n",
    "\n",
    "#####################################################################################################################################################\n",
    "#####################################################################################################################################################\n",
    "\n",
    "columns_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "data_toxic, data_severe_toxic, data_obscene, data_threat, data_insult, data_identity_hate = split_data(data)  \n",
    "\n",
    "train_data = data_threat.iloc[ :100 ].reset_index(drop=True)\n",
    "test_data  = data_threat.iloc[100: ].reset_index(drop=True)\n",
    "\n",
    "dataset_train = TrainData(train_data, data_target='threat', max_seq_len=max_seq_length)\n",
    "dataset_test  = TrainData(test_data,  data_target='threat', max_seq_len=max_seq_length)\n",
    "\n",
    "\n",
    "#####################################################################################################################################################\n",
    "#####################################################################################################################################################\n",
    "\n",
    "\n",
    "def collation_train(batch, vectorizer=dataset_train.vectorizer):\n",
    "    inputs = torch.stack([torch.stack([vectorizer(token) for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch]) \n",
    "    return inputs, target\n",
    "\n",
    "def collation_test(batch, vectorizer=dataset_test.vectorizer):\n",
    "    inputs = torch.stack([torch.stack([vectorizer(token) for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch]) \n",
    "    return inputs, target\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, collate_fn=collation_train, shuffle=True)\n",
    "test_loader  = DataLoader(dataset_test,  batch_size=32, collate_fn=collation_test)\n",
    "\n",
    "#####################################################################################################################################################\n",
    "#####################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "\n",
    "emb_dim = 300\n",
    "\n",
    "model = ToxicClassifier(max_seq_len=max_seq_length, emb_dim=emb_dim, hidden=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "all_train_losses, all_test_losses, all_accuracies = [],  [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "     train_losses, test_losses, running_accuracy = 0, 0, 0\n",
    "\n",
    "     for i, (sentences_train, labels_train) in enumerate(iter(train_loader)):\n",
    "          \n",
    "          i = i.to(device)\n",
    "          sentences_train = sentences_train.to(device)\n",
    "          labels_train = labels_train.to(device)\n",
    "\n",
    "          # print(sentences_train.shape)\n",
    "          sentences_train.resize_(sentences_train.size()[0], max_seq_length * emb_dim)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          prediction_train = model.forward(sentences_train)   \n",
    "          loss_train = criterion(prediction_train, labels_train) \n",
    "          loss_train.backward()                  \n",
    "          optimizer.step()                \n",
    "\n",
    "          train_losses += loss_train.item()\n",
    "     \n",
    "     avg_train_loss = train_losses/len(train_loader)\n",
    "     all_train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "     model.eval()\n",
    "     with torch.no_grad():\n",
    "          for i, (sentences_test, labels_test) in enumerate(iter(test_loader)):\n",
    "\n",
    "               i = i.to(device)\n",
    "               sentences_test = sentences_test.to(device)\n",
    "               labels_test = labels_test.to(device)\n",
    "               sentences_test.resize_(sentences_test.size()[0], max_seq_length * emb_dim)\n",
    "\n",
    "               prediction_test = model.forward(sentences_test) \n",
    "               loss_test = criterion(prediction_test, labels_test) \n",
    "\n",
    "               test_losses += loss_test.item()\n",
    "\n",
    "\n",
    "               prediction_class = torch.argmax(prediction_test, dim=1)\n",
    "               running_accuracy += accuracy_score(labels_test, prediction_class)\n",
    "          \n",
    "          avg_test_loss = test_losses/len(test_loader)\n",
    "          all_test_losses.append(avg_test_loss)\n",
    "\n",
    "          avg_running_accuracy = running_accuracy/len(test_loader)\n",
    "          all_accuracies.append(avg_running_accuracy)\n",
    "\n",
    "\n",
    "     model.train()\n",
    "\n",
    "\n",
    "     print(f'Epoch  : {e+1:3}/{epochs}    |   Train Loss:  : {avg_train_loss:.8f}     |  Test Loss:  : {avg_test_loss:.8f}  |  Accuracy  :   {avg_running_accuracy:.4f}')\n",
    "\n",
    "torch.save({ \"model_state\": model.to('cpu').state_dict(), 'max_seq_len' : 64, 'emb_dim' : 64, 'hidden1' : 32, 'hidden2' : 32}, 'trained_model_THREAT2')\n",
    "\n",
    "plt.plot(all_train_losses, label='Train Loss')\n",
    "plt.plot(all_test_losses,  label='Test Loss')\n",
    "plt.plot(all_accuracies,   label='Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
