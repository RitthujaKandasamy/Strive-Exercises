{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential with module and forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork(Features, hidden, target):\n",
    "\n",
    "\t# construct a sequential neural network\n",
    "\tmlpModel = nn.Sequential(OrderedDict([\n",
    "\t\t(\"hidden_layer\", nn.Linear(Features, hidden)),\n",
    "\t\t(\"activation_function\", nn.Sigmoid()),\n",
    "\t\t(\"output_layer\", nn.Linear(hidden, target)),\n",
    "\t\t(\"out\", nn.Sigmoid())                                       # we use this to get values between 0 and 1\n",
    "\t]))\n",
    "\t\n",
    "\treturn mlpModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.78051</td>\n",
       "      <td>-0.063669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28774</td>\n",
       "      <td>0.291390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40714</td>\n",
       "      <td>0.178780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29230</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50922</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.77029</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.73156</td>\n",
       "      <td>0.717820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.44556</td>\n",
       "      <td>0.579910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.85275</td>\n",
       "      <td>0.859870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.51912</td>\n",
       "      <td>0.623590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  2\n",
       "0   0.78051 -0.063669  1\n",
       "1   0.28774  0.291390  1\n",
       "2   0.40714  0.178780  1\n",
       "3   0.29230  0.421700  1\n",
       "4   0.50922  0.352560  1\n",
       "..      ...       ... ..\n",
       "95  0.77029  0.701400  0\n",
       "96  0.73156  0.717820  0\n",
       "97  0.44556  0.579910  0\n",
       "98  0.85275  0.859870  0\n",
       "99  0.51912  0.623590  0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y with tensor\n",
    "X = torch.tensor(data.drop(2, axis=1).values, dtype= torch.float)\n",
    "y = torch.tensor(data[2].values, dtype= torch.float).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 2])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork(X.shape[1], 10, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, train loss 0.69\n",
      "Epoch 500, train loss 0.69\n",
      "Epoch 500, train loss 0.69\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.68\n",
      "Epoch 500, train loss 0.67\n",
      "Epoch 500, train loss 0.67\n",
      "Epoch 500, train loss 0.67\n",
      "Epoch 500, train loss 0.67\n",
      "Epoch 500, train loss 0.67\n",
      "Epoch 500, train loss 0.67\n",
      "Epoch 500, train loss 0.66\n",
      "Epoch 500, train loss 0.66\n",
      "Epoch 500, train loss 0.66\n",
      "Epoch 500, train loss 0.66\n",
      "Epoch 500, train loss 0.66\n",
      "Epoch 500, train loss 0.65\n",
      "Epoch 500, train loss 0.65\n",
      "Epoch 500, train loss 0.65\n",
      "Epoch 500, train loss 0.65\n",
      "Epoch 500, train loss 0.65\n",
      "Epoch 500, train loss 0.64\n",
      "Epoch 500, train loss 0.64\n",
      "Epoch 500, train loss 0.64\n",
      "Epoch 500, train loss 0.64\n",
      "Epoch 500, train loss 0.64\n",
      "Epoch 500, train loss 0.63\n",
      "Epoch 500, train loss 0.63\n",
      "Epoch 500, train loss 0.63\n",
      "Epoch 500, train loss 0.63\n",
      "Epoch 500, train loss 0.62\n",
      "Epoch 500, train loss 0.62\n",
      "Epoch 500, train loss 0.62\n",
      "Epoch 500, train loss 0.61\n",
      "Epoch 500, train loss 0.61\n",
      "Epoch 500, train loss 0.61\n",
      "Epoch 500, train loss 0.61\n",
      "Epoch 500, train loss 0.60\n",
      "Epoch 500, train loss 0.60\n",
      "Epoch 500, train loss 0.60\n",
      "Epoch 500, train loss 0.59\n",
      "Epoch 500, train loss 0.59\n",
      "Epoch 500, train loss 0.59\n",
      "Epoch 500, train loss 0.58\n",
      "Epoch 500, train loss 0.58\n",
      "Epoch 500, train loss 0.57\n",
      "Epoch 500, train loss 0.57\n",
      "Epoch 500, train loss 0.57\n",
      "Epoch 500, train loss 0.56\n",
      "Epoch 500, train loss 0.56\n",
      "Epoch 500, train loss 0.56\n",
      "Epoch 500, train loss 0.55\n",
      "Epoch 500, train loss 0.55\n",
      "Epoch 500, train loss 0.54\n",
      "Epoch 500, train loss 0.54\n",
      "Epoch 500, train loss 0.53\n",
      "Epoch 500, train loss 0.53\n",
      "Epoch 500, train loss 0.53\n",
      "Epoch 500, train loss 0.52\n",
      "Epoch 500, train loss 0.52\n",
      "Epoch 500, train loss 0.51\n",
      "Epoch 500, train loss 0.51\n",
      "Epoch 500, train loss 0.50\n",
      "Epoch 500, train loss 0.50\n",
      "Epoch 500, train loss 0.50\n",
      "Epoch 500, train loss 0.49\n",
      "Epoch 500, train loss 0.49\n",
      "Epoch 500, train loss 0.48\n",
      "Epoch 500, train loss 0.48\n",
      "Epoch 500, train loss 0.47\n",
      "Epoch 500, train loss 0.47\n",
      "Epoch 500, train loss 0.46\n",
      "Epoch 500, train loss 0.46\n",
      "Epoch 500, train loss 0.45\n",
      "Epoch 500, train loss 0.45\n",
      "Epoch 500, train loss 0.44\n",
      "Epoch 500, train loss 0.44\n",
      "Epoch 500, train loss 0.44\n",
      "Epoch 500, train loss 0.43\n",
      "Epoch 500, train loss 0.43\n",
      "Epoch 500, train loss 0.42\n",
      "Epoch 500, train loss 0.42\n",
      "Epoch 500, train loss 0.41\n",
      "Epoch 500, train loss 0.41\n",
      "Epoch 500, train loss 0.41\n",
      "Epoch 500, train loss 0.40\n",
      "Epoch 500, train loss 0.40\n",
      "Epoch 500, train loss 0.39\n",
      "Epoch 500, train loss 0.39\n",
      "Epoch 500, train loss 0.38\n",
      "Epoch 500, train loss 0.38\n",
      "Epoch 500, train loss 0.38\n",
      "Epoch 500, train loss 0.37\n",
      "Epoch 500, train loss 0.37\n",
      "Epoch 500, train loss 0.36\n",
      "Epoch 500, train loss 0.36\n",
      "Epoch 500, train loss 0.36\n",
      "Epoch 500, train loss 0.35\n",
      "Epoch 500, train loss 0.35\n",
      "Epoch 500, train loss 0.35\n",
      "Epoch 500, train loss 0.34\n",
      "Epoch 500, train loss 0.34\n",
      "Epoch 500, train loss 0.34\n",
      "Epoch 500, train loss 0.33\n",
      "Epoch 500, train loss 0.33\n",
      "Epoch 500, train loss 0.33\n",
      "Epoch 500, train loss 0.32\n",
      "Epoch 500, train loss 0.32\n",
      "Epoch 500, train loss 0.32\n",
      "Epoch 500, train loss 0.31\n",
      "Epoch 500, train loss 0.31\n",
      "Epoch 500, train loss 0.31\n",
      "Epoch 500, train loss 0.30\n",
      "Epoch 500, train loss 0.30\n",
      "Epoch 500, train loss 0.30\n",
      "Epoch 500, train loss 0.30\n",
      "Epoch 500, train loss 0.29\n",
      "Epoch 500, train loss 0.29\n",
      "Epoch 500, train loss 0.29\n",
      "Epoch 500, train loss 0.29\n",
      "Epoch 500, train loss 0.28\n",
      "Epoch 500, train loss 0.28\n",
      "Epoch 500, train loss 0.28\n",
      "Epoch 500, train loss 0.28\n",
      "Epoch 500, train loss 0.27\n",
      "Epoch 500, train loss 0.27\n",
      "Epoch 500, train loss 0.27\n",
      "Epoch 500, train loss 0.27\n",
      "Epoch 500, train loss 0.27\n",
      "Epoch 500, train loss 0.26\n",
      "Epoch 500, train loss 0.26\n",
      "Epoch 500, train loss 0.26\n",
      "Epoch 500, train loss 0.26\n",
      "Epoch 500, train loss 0.26\n",
      "Epoch 500, train loss 0.26\n",
      "Epoch 500, train loss 0.25\n",
      "Epoch 500, train loss 0.25\n",
      "Epoch 500, train loss 0.25\n",
      "Epoch 500, train loss 0.25\n",
      "Epoch 500, train loss 0.25\n",
      "Epoch 500, train loss 0.25\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.24\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.23\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.22\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.21\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.20\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.19\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n",
      "Epoch 500, train loss 0.18\n"
     ]
    }
   ],
   "source": [
    "trainloss = []                   \n",
    "epochs = 500\n",
    "\n",
    "# pass the data through the model for a number of epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # put model in training model\n",
    "    model.train()\n",
    "    # forward pass on train data using the forward() method inside\n",
    "    y_pred = model(X_train)\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # zero the gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # perform backpropagation on the loss\n",
    "    loss.backward()\n",
    "    # progress the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # store loss \n",
    "    trainloss.append(loss.detach().item())\n",
    "    print(f'Epoch {epochs}, train loss {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_layer): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (activation_function): Sigmoid()\n",
       "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 501, train loss: 0.18, test loss: 0.05, accuracy: [tensor(0.9667)]\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "            test_preds = model.forward(X_test)\n",
    "            test_loss = loss_fn(test_preds, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            # to get prediction values between 0 and 1\n",
    "            out_values = test_preds > 0.5\n",
    "\n",
    "            # to check test accuracy\n",
    "            #accuracy = (out_values == y_test).sum() / out_values.shape[0]\n",
    "            accuracy = (out_values == y_test).sum() / len(y_test)\n",
    "            test_acc.append(accuracy)\n",
    "print(f'Epoch: {epochs + 1}, train loss: {loss.item():.2f}, test loss: {test_loss.item():.2f}, accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\01. MLP Binary Classification\\binary.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/binary.ipynb#ch0000025?line=23'>24</a>\u001b[0m \u001b[39m#print(f'Epoch: {epoch + 1}, train loss: {trainloss:.2f}, test loss: {test_losses:.2f}, accuracy: {test_acc:.2f}')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/binary.ipynb#ch0000025?line=24'>25</a>\u001b[0m \u001b[39m#print(trainloss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/binary.ipynb#ch0000025?line=25'>26</a>\u001b[0m \u001b[39m# display model progress on the current training batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/binary.ipynb#ch0000025?line=26'>27</a>\u001b[0m trainTemplate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m train loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m train accuracy: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/binary.ipynb#ch0000025?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(trainTemplate\u001b[39m.\u001b[39;49mformat(epoch \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,(trainloss), (test_acc)))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "source": [
    "trainloss = []\n",
    "test_losses = []\n",
    "test_acc = []\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    trainloss.append(loss.detach().item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            test_preds = model.forward(X_test)\n",
    "            test_loss = loss_fn(test_preds, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "            out_values = test_preds > 0.5\n",
    "            accuracy = (out_values == y_test).sum() / len(y_test)\n",
    "            test_acc.append(accuracy.item())\n",
    "    model.train()\n",
    "#print(f'Epoch: {epoch + 1}, train loss: {trainloss:.2f}, test loss: {test_losses:.2f}, accuracy: {test_acc:.2f}')\n",
    "#print(trainloss)\n",
    "# display model progress on the current training batch\n",
    "trainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
    "print(trainTemplate.format(epoch + 1,(trainloss), (test_acc))) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
