{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hazardous-weather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:47:37.551793Z",
     "start_time": "2021-05-26T07:47:35.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "clean-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:50:32.482086Z",
     "start_time": "2021-05-26T07:50:32.456893Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a neural network class inheriting from the nn.Module\n",
    "# Call it NeuralNetwork and make, and use \"pass\" in the constructor\n",
    "# so that it doesn't give an error\n",
    "# Instantiate one instance of it in variable net\n",
    "\n",
    "net = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        pass\n",
    "\n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "demographic-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:51:28.420569Z",
     "start_time": "2021-05-26T07:51:28.412916Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(net, NeuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "curious-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:11.203531Z",
     "start_time": "2021-05-26T07:56:11.199729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim and num_hidden, respectively the dimension of \n",
    "# the input and the number of hidden neurons\n",
    "# use pass again\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    pass\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super(NeuralNetwork).__init__()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "recreational-macro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:32.252906Z",
     "start_time": "2021-05-26T07:56:32.247913Z"
    }
   },
   "outputs": [],
   "source": [
    "assert NeuralNetwork(input_dim=10, num_hidden=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bigger-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:27.491588Z",
     "start_time": "2021-05-26T08:04:27.484159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim, num_hidden1 and num_hidden2, respectively the dimension of \n",
    "# the input and the number of hidden neurons for the first fully connected\n",
    "# layer and the second. Define the attributes in the constructor\n",
    "# that consists of the layers, call them fc1, fc2 and fc3 and a sigmoid.\n",
    "# use pass again. Be careful to put the dimensions in the right places!\n",
    "# Since we will do a binary classification problem, fc3 will have 1 neuron\n",
    "# as output\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden1)\n",
    "        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.fc3 = nn.Linear(num_hidden2, 1)      # 1 is for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layer1 = self.fc1(x)                      # layer = W.X + b\n",
    "        act1 = self.sigmoid(layer1)               # activated = sigmoid(layer)\n",
    "        layer2 = self.fc2(act1)\n",
    "        act2 = self.sigmoid(layer2)\n",
    "        layer3 = self.fc3(act2)\n",
    "        out = self.sigmoid(layer3)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hawaiian-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:48.612004Z",
     "start_time": "2021-05-26T08:04:48.606773Z"
    }
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(16, 16, 16)\n",
    "assert net.fc1\n",
    "assert net.fc2\n",
    "assert net.fc3\n",
    "assert net.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "smart-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass to make a reasonable use of the attributes\n",
    "# you defined before. Follow the same reasoning we used in class\n",
    "\n",
    "model = NeuralNetwork(10, 7, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "933260ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000025125E74D60>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()                      # see weihgt with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "latest-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training a model, use the following optimizer and loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)           # help to decreses learning rate (lr)\n",
    "\n",
    "# BCE is Binary Cross Entropy\n",
    "loss = nn.BCELoss()                                                 # loss function to get min gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lesser-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a neural network (feel free to choose the num_hidden1 and num_hidden2)\n",
    "# on the dataset in data.csv file\n",
    "# You'll have fun with conflicting shapes and types and tensors, but\n",
    "# you'll get those errors anyway. Let's go into the wild and learn\n",
    "# by reading the errors and trying to understand them! :)\n",
    "# You can always use the provided Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d21fb003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.78051</td>\n",
       "      <td>-0.063669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28774</td>\n",
       "      <td>0.291390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40714</td>\n",
       "      <td>0.178780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29230</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50922</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.77029</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.73156</td>\n",
       "      <td>0.717820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.44556</td>\n",
       "      <td>0.579910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.85275</td>\n",
       "      <td>0.859870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.51912</td>\n",
       "      <td>0.623590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  2\n",
       "0   0.78051 -0.063669  1\n",
       "1   0.28774  0.291390  1\n",
       "2   0.40714  0.178780  1\n",
       "3   0.29230  0.421700  1\n",
       "4   0.50922  0.352560  1\n",
       "..      ...       ... ..\n",
       "95  0.77029  0.701400  0\n",
       "96  0.73156  0.717820  0\n",
       "97  0.44556  0.579910  0\n",
       "98  0.85275  0.859870  0\n",
       "99  0.51912  0.623590  0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43366476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "643eef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y with tensor\n",
    "X = torch.tensor(data.drop(2, axis=1).values, dtype= torch.float)\n",
    "y = torch.tensor(data[2].values, dtype= torch.float).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98239f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee233c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "netmodel = NeuralNetwork(10, 10, 9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2172562d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=9, bias=True)\n",
       "  (fc3): Linear(in_features=9, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bff78ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f367e2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x2 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\01. MLP Binary Classification\\BinaryClassification_Exercises.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000019?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000019?line=6'>7</a>\u001b[0m \u001b[39m# forward pass on train data using the forward() method inside\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000019?line=7'>8</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000019?line=8'>9</a>\u001b[0m \u001b[39m# calaculate the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000019?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\01. MLP Binary Classification\\BinaryClassification_Exercises.ipynb Cell 6'\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000005?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000005?line=18'>19</a>\u001b[0m     layer1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)                      \u001b[39m# layer = W.X + b\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000005?line=19'>20</a>\u001b[0m     act1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(layer1)               \u001b[39m# activated = sigmoid(layer)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/01.%20MLP%20Binary%20Classification/BinaryClassification_Exercises.ipynb#ch0000005?line=20'>21</a>\u001b[0m     layer2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(act1)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x2 and 10x10)"
     ]
    }
   ],
   "source": [
    "losses = []                   \n",
    "epochs = 500\n",
    "# pass the data through the model for a number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # put model in training model\n",
    "    model.train()\n",
    "    # forward pass on train data using the forward() method inside\n",
    "    y_pred = model(X)\n",
    "    # calaculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # zero the gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # perform backpropagation on the loss\n",
    "    loss.backward()\n",
    "    # progress the optimizer\n",
    "    optimizer.step()\n",
    "    # store loss \n",
    "    losses.append(loss.detach())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
