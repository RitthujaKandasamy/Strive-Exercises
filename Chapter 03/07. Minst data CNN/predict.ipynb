{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datahandler import testloader\n",
    "from model import ConvNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model_state = torch.load('model.pth')\n",
    "model = ConvNet()\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize predictions\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze(), cmap = 'Greys_r')\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26564ba8970>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('images\\\\6.png')\n",
    "img = cv2.resize(img, (28, 28))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "retval, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "img = np.invert(img)\n",
    "\n",
    "img = img/255\n",
    "img = img.astype('float64')\n",
    "\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, str, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !str!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, str, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\07. Minst data CNN\\predict.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/predict.ipynb#ch0000006?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/predict.ipynb#ch0000006?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/predict.ipynb#ch0000006?line=3'>4</a>\u001b[0m     log_soft \u001b[39m=\u001b[39m model((\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49mimg[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/predict.ipynb#ch0000006?line=4'>5</a>\u001b[0m ps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(log_soft)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/predict.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39m#view_classify(img, ps)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\07. Minst data CNN\\model.py:28\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/model.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='file:///c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/model.py?line=27'>28</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/model.py?line=28'>29</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/07.%20Minst%20data%20CNN/model.py?line=30'>31</a>\u001b[0m     \u001b[39m# flatten\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, str, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !str!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, str, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_soft = model(img.view(1, *img[0].shape))\n",
    "ps = torch.exp(log_soft)\n",
    "#view_classify(img, ps)\n",
    "print(np.argmax(ps))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
