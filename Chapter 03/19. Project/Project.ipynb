{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems facing in this project are:\n",
    "\n",
    "* Dataset has only train data --> so we need to create the train and test\n",
    "* It's maxiumum accuracy is 60% only --> so we create critical model\n",
    "* Total classes 121 and Total images 30k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating csv file to gather data of images \n",
    "\n",
    "def generate_csv(root, img_ext = ['jpg', 'png', 'jpeg']):\n",
    "\n",
    "    # create a dataframe to store the data we get from the file\n",
    "    df = pd.DataFrame(columns = ['path', 'labels'])\n",
    "\n",
    "    # os.listdir gives the folder name of the files\n",
    "    for index, label in enumerate(os.listdir(root)):\n",
    "\n",
    "            links = glob(f\"{root}/{label}/*{img_ext}\")           # glob used to get all information\n",
    "            #print(len(links))\n",
    "\n",
    "            # np.ones gives 1 to total number of links and multiply with index which starts from 0\n",
    "            # np.ones(4)*3 = array([3, 3, 3, 3])\n",
    "            # so by this we get both path for images and the labels in numbers till(0 - 120)\n",
    "            temp_df = pd.DataFrame({'path': links, 'labels': np.ones(len(links))*index})\n",
    "\n",
    "            df = pd.concat([df, temp_df], axis = 0)      # merge all in one\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30336 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path labels\n",
       "0    C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...    0.0\n",
       "1    C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...    0.0\n",
       "2    C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...    0.0\n",
       "3    C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...    0.0\n",
       "4    C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...    0.0\n",
       "..                                                 ...    ...\n",
       "420  C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...  120.0\n",
       "421  C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...  120.0\n",
       "422  C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...  120.0\n",
       "423  C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...  120.0\n",
       "424  C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train...  120.0\n",
       "\n",
       "[30336 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_csv('C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24268, 2), (6068, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, test_ds = train_test_split(data, test_size = 0.2, random_state = 0, stratify = data[\"labels\"])\n",
    "train_ds, test_ds = train_ds.reset_index(drop=True), test_ds.reset_index(drop=True)\n",
    "train_ds.shape, test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/acantharia_protist\\\\100224.jpg',\n",
       "       'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/acantharia_protist\\\\100723.jpg',\n",
       "       'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/acantharia_protist\\\\101165.jpg',\n",
       "       ...,\n",
       "       'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/unknown_unclassified\\\\9909.jpg',\n",
       "       'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/unknown_unclassified\\\\99593.jpg',\n",
       "       'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/unknown_unclassified\\\\99703.jpg'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"path\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new folder to store train and test in desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_folder_train = \"C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train_new\"\n",
    "dest_folder_test = \"C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\test_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(dest_folder, data):\n",
    "\n",
    "    for pth in data.values[:, 0]:            # take the path to convert into images\n",
    "\n",
    "        # 'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\datasciencebowl\\\\train\\\\train/siphonophore_physonect\\\\14249.jpg'\n",
    "        # pth.split('/')[-1] = 'siphonophore_physonect\\\\14249.jpg'\n",
    "        # ['siphonophore_physonect', '14249.jpg']\n",
    "        folder_img = pth.split(\"/\")[-1].split(\"\\\\\")     \n",
    "        folder, img = folder_img[0], folder_img[1]\n",
    "\n",
    "\n",
    "        # folder join with the path Eg:[C:\\Users\\ritth\\code\\Data\\datasciencebowl\\train_new\\trichodesmium_tuft]\n",
    "        label_folder = os.path.join(dest_folder, folder)\n",
    "\n",
    "        if not os.path.isdir(label_folder):                # if the folder does not exist \n",
    "            os.mkdir(label_folder)                         # creating the folder \n",
    "        shutil.copy(pth , label_folder)                    # copy the content of source(images) to destination(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_files(dest_folder_train, train_ds)\n",
    "copy_files(dest_folder_test, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                                    transforms.Resize((50, 50)),           # resize will resize all the images into same scale(same pixels) given images size are small and big, so we take approximitily 50\n",
    "                                    transforms.RandomResizedCrop(28),      # crop adjust the images to other images in features and we taken it as 28 smaller than resize\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(\n",
    "                                        mean=[0.485, 0.456, 0.406],        # normalize have mean and standard deviation for color pic (red, green, blue)\n",
    "                                        std=[0.229, 0.224, 0.225]) \n",
    "                                    ])\n",
    "    \n",
    "    \n",
    "test_transform = transforms.Compose([\n",
    "                                    transforms.Resize((50, 50)),\n",
    "                                    transforms.CenterCrop(28),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(\n",
    "                                        mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "trainset = datasets.ImageFolder(dest_folder_train, transform = train_transform)\n",
    "trainloader = DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "\n",
    "\n",
    "# Load the test data\n",
    "testset = datasets.ImageFolder(dest_folder_test,transform = test_transform)\n",
    "testloader = DataLoader(testset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnext101_32x8d(pretrained = True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnext101_32x8d(pretrained = True)\n",
    "\n",
    "inputs = model.fc.in_features\n",
    "outputs = len(trainset.classes)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "clf = nn.Sequential( \n",
    "              nn.Linear(inputs, outputs)\n",
    "                  )\n",
    "\n",
    "model.fc = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "benchmark_accuracy = 0.35\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    running_accuracy = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    # training\n",
    "    for x_train_batch, y_train_batch in trainloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        logits = model.forward(x_train_batch)\n",
    "        train_preds = torch.argmax(logits.detach(), dim=1)\n",
    "\n",
    "        # loss\n",
    "        train_loss = criterion(logits, y_train_batch)\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        train_acc = (y_train_batch == train_preds).sum() / len(y_train_batch)\n",
    "        running_accuracy += train_acc.item()\n",
    "\n",
    "        # backward pass\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # mean loss (all batches losses divided by the total number of batches)\n",
    "    train_losses.append(running_loss / len(trainloader))\n",
    "    \n",
    "    # mean accuracies\n",
    "    train_accuracies.append(running_accuracy / len(trainloader))\n",
    "    \n",
    "    # print\n",
    "    print(f'Train loss: {train_losses[-1] :.4f}')\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_accuracy = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        for x_test_batch, y_test_batch in testloader:\n",
    "            \n",
    "            # logits\n",
    "            test_logits = model(x_test_batch)\n",
    "\n",
    "            # predictions\n",
    "            test_preds = torch.argmax(test_logits, dim=1)\n",
    "            \n",
    "            # accuracy\n",
    "            test_acc = (y_test_batch == test_preds).sum() / len(y_test_batch)\n",
    "            running_accuracy += test_acc.item()\n",
    "            \n",
    "            # loss\n",
    "            test_loss = criterion(test_logits, y_test_batch)\n",
    "            running_loss += test_loss.item()\n",
    "\n",
    "        # mean accuracy for each epoch\n",
    "        test_accuracies.append(running_accuracy / len(testloader))\n",
    "\n",
    "        # mean loss for each epoch\n",
    "        test_losses.append(running_accuracy / len(testloader))\n",
    "\n",
    "        # print\n",
    "        print(f'Test accuracy: {test_accuracies[-1]*100 :.2f}%')\n",
    "        print('='*100)\n",
    "\n",
    "        # saving best model\n",
    "        # is current mean score (mean per epoch) greater than or equal to the benchmark?\n",
    "        if test_accuracies[-1] > benchmark_accuracy:\n",
    "            \n",
    "            # save model \n",
    "            torch.save(model.state_dict(), './model.pth')\n",
    "\n",
    "            # update benckmark\n",
    "            benchmark_accuracy = test_accuracies[-1]\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "# Plots\n",
    "x_epochs = list(range(epochs))\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_epochs, train_losses, marker='o', label='train')\n",
    "plt.plot(x_epochs, test_losses, marker='o', label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_epochs, train_accuracies, marker='o', label='train')\n",
    "plt.plot(x_epochs, test_accuracies, marker='o', label='test')\n",
    "plt.axhline(benchmark_accuracy, c='grey', ls='--',\n",
    "            label=f'Best_accuracy({benchmark_accuracy*100 :.2f}%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./learning_curve.png', dpi = 200)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8abb0779bf248a90442b2ba375cb2dc444ddbc86bf34dd34983763988490020e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
