{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a transformation to normalize the dataset\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5), (0.5))\n",
    "                                ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST(\"~/.pytorch/F_MNIST_data/\", download=True, train=True, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST(\"~/.pytorch/F_MNIST_data/\", download=True, train=False, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Creating a function to visualize the dataset\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "  \"\"\"Imshow for Tensor.\"\"\"\n",
    "  if ax is None:\n",
    "      fig, ax = plt.subplots()\n",
    "  image = image.numpy().transpose((1, 2, 0))\n",
    "   \n",
    "  if normalize:\n",
    "      mean = np.array([0.485, 0.456, 0.406])\n",
    "      std = np.array([0.229, 0.224, 0.225])\n",
    "      image = std * image + mean\n",
    "      image = np.clip(image, 0, 1)\n",
    "\n",
    "  ax.imshow(image)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['left'].set_visible(False)\n",
    "  ax.spines['bottom'].set_visible(False)\n",
    "  ax.tick_params(axis='both', length=0)\n",
    "  ax.set_xticklabels('')\n",
    "  ax.set_yticklabels('')\n",
    "\n",
    "  return ax\n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "imshow(image[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x25740272610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 8, 6, 8, 3, 0, 7, 7, 2, 2, 7, 5, 8, 4, 3, 1, 8, 5, 6, 6, 1, 8, 7, 3,\n",
       "        9, 1, 4, 3, 5, 5, 5, 5, 8, 5, 3, 9, 0, 9, 2, 1, 0, 7, 7, 1, 8, 6, 4, 9,\n",
       "        4, 9, 2, 7, 3, 6, 2, 3, 1, 2, 3, 9, 3, 7, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2574057faf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[63].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Hyperparameters for our network\n",
    "# input_size   = 784\n",
    "# hidden_sizes = [256, 128, 64]\n",
    "# output_size   = 10\n",
    "\n",
    "# # Build a feed-forward network\n",
    "# model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "#                       nn.Dropout(0.2),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#                       nn.Dropout(0.2),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "#                       nn.Dropout(0.2),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(hidden_sizes[2], output_size),\n",
    "#                       nn.Softmax(dim=1))                          # dim is dimensionality of softmax\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with a 0.2 drop probability \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(x.shape[0], -1)    \n",
    "        # Set the activation functions\n",
    "        layer1 = self.dropout(F.relu(self.fc1(x)))\n",
    "        layer2 = self.dropout(F.relu(self.fc2(layer1)))\n",
    "        layer3 = self.dropout(F.relu(self.fc3(layer2)))\n",
    "        out = F.log_softmax(self.fc4(layer3), dim=1)\n",
    "    \n",
    "        return out\n",
    "    \n",
    "model = Network()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize =(6, 9), ncols =2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    #ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])                          # ps is probablity\n",
    "\n",
    "img = images[img_idx]\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss 0.006..  Test Loss 0.006..  Test Accuracy 86.520%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\05. MLP Kaggle challenge\\notebook.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=5'>6</a>\u001b[0m train_running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m trainloader:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=8'>9</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\05. MLP Kaggle challenge\\notebook.ipynb Cell 15\u001b[0m in \u001b[0;36mNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=13'>14</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=14'>15</a>\u001b[0m \u001b[39m# Set the activation functions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=15'>16</a>\u001b[0m layer1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=16'>17</a>\u001b[0m layer2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(layer1)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/05.%20MLP%20Kaggle%20challenge/notebook.ipynb#ch0000014?line=17'>18</a>\u001b[0m layer3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(layer2)))\n",
      "File \u001b[1;32mc:\\Users\\ritth\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ritth\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_losses, test_losses, test_accuracies = [], [], []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = 0 \n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    # validation    \n",
    "    model.eval()\n",
    "\n",
    "    # Turn off gradients for validation to save memory and speed up computations\n",
    "    with torch.no_grad():\n",
    "        test_running_loss = 0\n",
    "        acc = 0\n",
    "\n",
    "        for images, labels in testloader:\n",
    "            testoutput = model.forward(images)\n",
    "            testloss = criterion(testoutput, labels)\n",
    "            test_running_loss += testloss.item()\n",
    "\n",
    "            ps = torch.exp(testoutput)\n",
    "            top_p, top_class = ps.topk(1, dim=1)                        # top_p = probability, top_class = index\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            acc += equals.sum().item()\n",
    "            \n",
    "    model.train()\n",
    "    \n",
    "    # Get mean loss to enable comparison between train and test sets\n",
    "    train_loss = train_running_loss/ len(trainloader.dataset)\n",
    "    test_loss = test_running_loss / len(testloader.dataset)\n",
    "    \n",
    "    # At completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # test accuracy\n",
    "    test_accuracies.append(acc / len(testloader.dataset))\n",
    "\n",
    "\n",
    "    print(\"Epoch: {}/{}.. \".format(epoch+1, num_epochs),\n",
    "        \"Training Loss {:.3f}.. \".format(train_loss),\n",
    "        \"Test Loss {:.3f}.. \".format(test_loss),\n",
    "        \"Test Accuracy {:.2f}%\".format(test_accuracies[-1]*100))\n",
    "\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_losses, marker='o', label='train')\n",
    "plt.plot(test_losses, marker='o', label='test')\n",
    "plt.plot(test_accuracies, marker='o', label='test_acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('./learning_curve1.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('checkpoint1.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "\n",
    "def load_model():\n",
    "  model = torch.load('checkpoint.pth')\n",
    "  return model\n",
    "   \n",
    "with st.spinner('Model is being loaded..'):\n",
    "  model = load_model()\n",
    "\n",
    "\n",
    "st.write(\"\"\"\n",
    "         # Dress Classification\n",
    "         \"\"\"\n",
    "         )\n",
    "\n",
    "file = st.file_uploader(\"Please upload an image file\", type=[\"jpg\", \"png\"])\n",
    "\n",
    "\n",
    "#import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
    "\n",
    "def import_and_predict(image_data, model):\n",
    "    \n",
    "        size = (28, 28)    \n",
    "        image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\n",
    "        image = np.asarray(image)\n",
    "        #img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #img_resize = (cv2.resize(img, dsize=(75, 75),    interpolation=cv2.INTER_CUBIC))/255.\n",
    "        \n",
    "        img_reshape = img[np.newaxis,...]\n",
    "    \n",
    "        prediction = model.predict(img_reshape)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "if file is None:\n",
    "    st.text(\"Please upload an image file\")\n",
    "else:\n",
    "    image = Image.open(file)\n",
    "    st.image(image, use_column_width=True)\n",
    "    predictions = import_and_predict(image, model)\n",
    "    score = torch.nn.Softmax(predictions[0])\n",
    "    class_names = ['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot']\n",
    "    st.write(predictions)\n",
    "    st.write(score)\n",
    "\n",
    "    print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8abb0779bf248a90442b2ba375cb2dc444ddbc86bf34dd34983763988490020e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
