{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Multiclass Clasification: MNIST</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxliary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/view-classify-in-module-helper/30279/6\n",
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize =(6, 9), ncols =2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    #ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)                  \n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=0.5, std=0.5)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: MNIST_data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=0.5, std=0.5)\n",
       "           )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: MNIST_data/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=0.5, std=0.5)\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x12a42956050>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 2, 7, 6, 6, 1, 1, 2, 7, 5, 8, 5, 3, 4, 2, 0, 9, 4, 0, 7, 7, 3,\n",
       "        1, 7, 1, 9, 6, 6, 5, 0, 1, 9, 4, 6, 8, 2, 3, 5, 2, 8, 8, 1, 0, 3, 8, 9,\n",
       "        9, 2, 2, 0, 4, 3, 4, 0, 2, 4, 4, 9, 2, 8, 4, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12a42bb84f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[63].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))                          # dim is dimensionality of softmax\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "          ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0190, -0.0298, -0.0259,  ..., -0.0328, -0.0307, -0.0299],\n",
      "        [-0.0336, -0.0158, -0.0222,  ..., -0.0183,  0.0181, -0.0114],\n",
      "        [ 0.0299,  0.0303,  0.0045,  ...,  0.0042, -0.0211,  0.0339],\n",
      "        ...,\n",
      "        [-0.0259,  0.0122, -0.0116,  ...,  0.0034,  0.0202,  0.0210],\n",
      "        [-0.0243, -0.0331, -0.0183,  ..., -0.0300,  0.0117, -0.0019],\n",
      "        [-0.0117,  0.0305, -0.0113,  ..., -0.0166,  0.0222,  0.0312]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0049,  0.0258,  0.0314, -0.0154, -0.0339, -0.0337, -0.0270, -0.0133,\n",
      "        -0.0266, -0.0234, -0.0320,  0.0205, -0.0259, -0.0009, -0.0019,  0.0307,\n",
      "        -0.0062, -0.0020,  0.0040, -0.0082,  0.0270,  0.0113, -0.0013, -0.0252,\n",
      "         0.0014,  0.0242,  0.0224, -0.0347, -0.0276,  0.0281,  0.0228,  0.0280,\n",
      "         0.0056, -0.0066,  0.0064, -0.0338,  0.0354,  0.0264,  0.0140, -0.0301,\n",
      "        -0.0277, -0.0325, -0.0349, -0.0124, -0.0306,  0.0113,  0.0166, -0.0088,\n",
      "        -0.0223,  0.0232, -0.0169,  0.0294, -0.0104,  0.0182,  0.0205, -0.0053,\n",
      "         0.0271, -0.0081,  0.0254,  0.0015, -0.0201,  0.0223, -0.0267,  0.0035,\n",
      "         0.0131,  0.0240, -0.0308,  0.0044, -0.0055, -0.0307,  0.0151, -0.0177,\n",
      "        -0.0322,  0.0122,  0.0301,  0.0303,  0.0205,  0.0010, -0.0314, -0.0142,\n",
      "        -0.0123, -0.0321,  0.0128, -0.0033,  0.0076, -0.0229, -0.0073,  0.0351,\n",
      "         0.0266, -0.0256, -0.0003, -0.0193, -0.0144,  0.0277,  0.0176,  0.0356,\n",
      "        -0.0165,  0.0319,  0.0230, -0.0223, -0.0172,  0.0202, -0.0284,  0.0151,\n",
      "         0.0010,  0.0027,  0.0182, -0.0235,  0.0065,  0.0049,  0.0065,  0.0087,\n",
      "        -0.0344, -0.0199,  0.0068, -0.0301, -0.0344,  0.0103,  0.0284, -0.0087,\n",
      "         0.0258, -0.0148,  0.0131, -0.0158, -0.0315,  0.0291, -0.0158, -0.0082],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0061,  0.0007, -0.0131,  ...,  0.0088,  0.0023,  0.0060],\n",
       "        [ 0.0002, -0.0034, -0.0017,  ...,  0.0074,  0.0159,  0.0123],\n",
       "        [-0.0092,  0.0199,  0.0124,  ...,  0.0025, -0.0031, -0.0096],\n",
       "        ...,\n",
       "        [ 0.0015, -0.0102, -0.0162,  ...,  0.0171,  0.0145, -0.0044],\n",
       "        [-0.0012,  0.0028, -0.0241,  ...,  0.0050, -0.0027, -0.0146],\n",
       "        [ 0.0026, -0.0014, -0.0245,  ...,  0.0009, -0.0212, -0.0072]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])                          # ps is probablity\n",
    "\n",
    "img = images[img_idx]\n",
    "view_classify(img.view(1, 28, 28), ps)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3179,  1.3372],\n",
      "        [-0.5652, -1.5970]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1010, 1.7881],\n",
      "        [0.3195, 2.5504]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x0000012A42FFA3B0>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1898, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1589,  0.6686],\n",
      "        [-0.2826, -0.7985]])\n",
      "tensor([[ 0.1589,  0.6686],\n",
      "        [-0.2826, -0.7985]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0302,  0.0216,  0.0057,  ...,  0.0189, -0.0314,  0.0299],\n",
      "        [ 0.0240,  0.0229, -0.0042,  ..., -0.0013,  0.0203,  0.0357],\n",
      "        [ 0.0287, -0.0229, -0.0332,  ..., -0.0190, -0.0032,  0.0333],\n",
      "        ...,\n",
      "        [-0.0113,  0.0195, -0.0258,  ..., -0.0237, -0.0062,  0.0135],\n",
      "        [-0.0230, -0.0324, -0.0188,  ...,  0.0284, -0.0066, -0.0086],\n",
      "        [ 0.0084,  0.0131, -0.0299,  ..., -0.0306,  0.0118,  0.0187]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        ...,\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0302,  0.0215,  0.0057,  ...,  0.0189, -0.0314,  0.0299],\n",
      "        [ 0.0240,  0.0229, -0.0042,  ..., -0.0013,  0.0203,  0.0357],\n",
      "        [ 0.0287, -0.0229, -0.0332,  ..., -0.0190, -0.0032,  0.0333],\n",
      "        ...,\n",
      "        [-0.0113,  0.0195, -0.0258,  ..., -0.0237, -0.0062,  0.0135],\n",
      "        [-0.0230, -0.0324, -0.0188,  ...,  0.0284, -0.0066, -0.0086],\n",
      "        [ 0.0084,  0.0131, -0.0298,  ..., -0.0306,  0.0118,  0.0187]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0577\n",
      "\tIteration: 40\t Loss: 2.2901\n",
      "\tIteration: 80\t Loss: 2.2719\n",
      "\tIteration: 120\t Loss: 2.2538\n",
      "\tIteration: 160\t Loss: 2.2365\n",
      "\tIteration: 200\t Loss: 2.2115\n",
      "\tIteration: 240\t Loss: 2.1935\n",
      "\tIteration: 280\t Loss: 2.1806\n",
      "\tIteration: 320\t Loss: 2.1443\n",
      "\tIteration: 360\t Loss: 2.1230\n",
      "\tIteration: 400\t Loss: 2.0907\n",
      "\tIteration: 440\t Loss: 2.0582\n",
      "\tIteration: 480\t Loss: 2.0117\n",
      "\tIteration: 520\t Loss: 1.9794\n",
      "\tIteration: 560\t Loss: 1.9281\n",
      "\tIteration: 600\t Loss: 1.8770\n",
      "\tIteration: 640\t Loss: 1.8208\n",
      "\tIteration: 680\t Loss: 1.7786\n",
      "\tIteration: 720\t Loss: 1.7033\n",
      "\tIteration: 760\t Loss: 1.6508\n",
      "\tIteration: 800\t Loss: 1.5676\n",
      "\tIteration: 840\t Loss: 1.5173\n",
      "\tIteration: 880\t Loss: 1.4546\n",
      "\tIteration: 920\t Loss: 1.3854\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0317\n",
      "\tIteration: 40\t Loss: 1.2850\n",
      "\tIteration: 80\t Loss: 1.2178\n",
      "\tIteration: 120\t Loss: 1.1744\n",
      "\tIteration: 160\t Loss: 1.1349\n",
      "\tIteration: 200\t Loss: 1.0717\n",
      "\tIteration: 240\t Loss: 0.9999\n",
      "\tIteration: 280\t Loss: 0.9916\n",
      "\tIteration: 320\t Loss: 0.9296\n",
      "\tIteration: 360\t Loss: 0.8857\n",
      "\tIteration: 400\t Loss: 0.8812\n",
      "\tIteration: 440\t Loss: 0.8212\n",
      "\tIteration: 480\t Loss: 0.8254\n",
      "\tIteration: 520\t Loss: 0.7772\n",
      "\tIteration: 560\t Loss: 0.7648\n",
      "\tIteration: 600\t Loss: 0.7597\n",
      "\tIteration: 640\t Loss: 0.7141\n",
      "\tIteration: 680\t Loss: 0.7083\n",
      "\tIteration: 720\t Loss: 0.7022\n",
      "\tIteration: 760\t Loss: 0.6892\n",
      "\tIteration: 800\t Loss: 0.6523\n",
      "\tIteration: 840\t Loss: 0.6588\n",
      "\tIteration: 880\t Loss: 0.6332\n",
      "\tIteration: 920\t Loss: 0.6231\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0124\n",
      "\tIteration: 40\t Loss: 0.6082\n",
      "\tIteration: 80\t Loss: 0.5661\n",
      "\tIteration: 120\t Loss: 0.6171\n",
      "\tIteration: 160\t Loss: 0.5691\n",
      "\tIteration: 200\t Loss: 0.5649\n",
      "\tIteration: 240\t Loss: 0.5817\n",
      "\tIteration: 280\t Loss: 0.5649\n",
      "\tIteration: 320\t Loss: 0.5943\n",
      "\tIteration: 360\t Loss: 0.5367\n",
      "\tIteration: 400\t Loss: 0.5241\n",
      "\tIteration: 440\t Loss: 0.5196\n",
      "\tIteration: 480\t Loss: 0.5374\n",
      "\tIteration: 520\t Loss: 0.5273\n",
      "\tIteration: 560\t Loss: 0.5307\n",
      "\tIteration: 600\t Loss: 0.4972\n",
      "\tIteration: 640\t Loss: 0.5087\n",
      "\tIteration: 680\t Loss: 0.4981\n",
      "\tIteration: 720\t Loss: 0.5208\n",
      "\tIteration: 760\t Loss: 0.4675\n",
      "\tIteration: 800\t Loss: 0.4719\n",
      "\tIteration: 840\t Loss: 0.5105\n",
      "\tIteration: 880\t Loss: 0.4546\n",
      "\tIteration: 920\t Loss: 0.4918\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):       # unpack data and index from trainloader, images is features and labels is target\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAroAAAGHCAYAAABf8fH3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABYlAAAWJQFJUiTwAAAqXUlEQVR4nO3deZgdZZn38e+dsMWwhEVABQkgGBBUEkX2VRkVRVxArxkYd3RkxAXfkVEZYZQRRh1BHEUEBMUZVBx0RBRQg6CAOgmiQJQ1CMoiYQuQsKTv94+qNofmnE5156TrVOX7ua66Kqfqqar7nK50//rpp6oiM5EkSZLaZlLdBUiSJEkrgkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiQgIrKcptddy8ogIuaXn/deTTluRBxTbntm1f1GxF7l8vnjq1jLw6ArSWqViHhaRPxDRHw/Iv4YEY9ExMMRcUtEnBsRh0TElLrrnCgdAaxzWhIRCyLisoj4QEQ8re46V0YRcWAZnvequ5a2WqXuAiRJ6peIeDVwKrBxx+KHgSFgejm9HjghIg7NzJ9OdI01ehh4qPz3asB6wG7l9I6I2Dsz766ruIa4B/gDcMcYtnmk3OZPXdYdCLy5/Pcly1OYurNHV5LUChHxFuC7FCH3D8ChwAaZuWZmrg1MA95AESieCexRR501+kxmblxO6wEbAMcBCWxL8QuCRpGZX8jMGZn5z2PY5lflNvuuyNrUnUFXktR4EfF84BSKn2sXADtk5tmZuWC4TWY+kJnfycy9gTcCC+updjBk5oLM/Bjw1XLRayLimXXWJPWbQVeS1AbHAatT/Hn4bzNz0WiNM/NbwH9U2XFETI6IvSPipIiYExF3RcRjEfHniDgvIvYZZdtJEfGWiJhdjol9PCL+EhHXRsQZEfHyLttsHhFfiojrI2JROcb41oi4JCL+OSI2qFL3GPx3x79ndtTx14vzImKbiDgrIm4r38N3R9S8Q0ScXa5/NCLuiYgLI+L1VQqIiGdHxGnl9ovL8dSfiYh1erRfLSL2j4ivRMTV5fEWl5/TNyJi1go6bs+L0UY5xlMuRhtextJhCx8fOY66bPcv5ev/W8Yx3lq2uy0izHYdHKMrSWq0iHgWsH/58vOZ+UCV7TIzKx5iG6BzLO+jwGPAMyjGWB4YER/NzH/rsu3Xgb/teP0AsDbFsIFty+lHwysjYibF0Iq1ykWPU4ytfXY57Qlc1blNH3SOHV27y/rdKXrLn0bRC/5E58qIOAz4Eks7z+6nGCayH7BfRJwNvCUzl/Q4/nOAbwFPpxhDnBRjqY+k6GXeIzNHjondD/h+x+tHyu2eTfF5HxwRb8vMr/c45niP2y+PAXcB6wBr8OTx053OAD4OzIqI7TPzdz3297ZyflZmDvW72CYz9UuSmm4vIMp//+8K2P9jwLeBV1OM/52SmWsCGwFHA0uAT0bESzo3iog9KELXEPABYO3MnEYRbJ4JvAX4+YhjfYYi5P4SmJmZq2XmusBU4MXAiRRhuZ+e3fHv+7us/yLwa2D7cqzz0yjCIBGxC0tD7rnApmW904CPUoTHQ4DRxrR+huI97Z6Za1G81wMpLvx6DnBWl20eohhysS/FOOypmTkF2IziM1oFODUint1l2+U5bl9k5uWZuTHwzeFaOsZPb1yuIzNvBy4s27y1274i4jkUFxQmS4ehqGTQlSQ13Tbl/FGKi9D6KjOvz8yDM/P8zLxruCc4M+/OzE8Cx1IE7XeP2HSncn5RZp6YmQvL7TIz78jMszLzQz22eV9mXtVRwyOZ+X+Z+YHMvKLPb/Gdw4ehCLQj3Q28IjOv6aj/pnLdJyiyxC+AN5XBjMx8qOzhPr5s9+GI6NZbDMWQk1dk5s/LbYcy83vAweX6l0XEbp0bZOYlmfm2zPzpiHHYf8zMD1D0hK5Bj3A43uPW5Cvl/JCIWLXL+uHe3Es7vi4qGXQlSU23fjm/bwzDEfpp+E/ou45Y/mA533AM4yaHt3nGclc1inKM67YRcRrF7dYAzsnMv3Rp/oVuY54jYj1g7/Llp3oMTTgBWAysCbyyRznfyswbRy7MzNnA5eXLN/R+N131+pqs6OOuCN+nGObwdOBVnSvK8+rvy5dnTHBdjWDQlSRpGSJiShQPVrgkIu4uL8gavmhouOd15B0Lfkwx7GEmcEkUD6pY1l0NLijnX4uI4yNipx69eOPx8Y6aHwWuBd5errsSeE+P7Xr1IO9A0ZOdwM+6NSjHS88pX87s1obR7x87vN+nbBsR60XE0RFxeXmh3xMd7++8stlon/e4jjvRMvMJlg6jGNlD/TfAsyh+QTp3IutqCi9GkyQ13fCfrteNiOh3r25EPIMiFG3dsfhh4D6K8beTKS4um9q5XWbeGBH/AHyB4oKu3cv9zae4mOzUzuEJpf8HPBfYBfhwOS2OiCsoxgmfuaw7Soyi84KnJRTjU+dRhMJzykDVTbdeXih6GAEeyMxuF1INu31E+5G6PUhh5LonbRsR21JcILhRx+KFwCKK4L0aMDy2eVn7rnzcGp0G/BPwiojYKDPvKpcPD1s4JzMfqae0wWaPriSp6eaV89UpQmK/nUgRcm+m+DP/euVDKDYsLxraqdeGmXkGsDnwfuB7FKF8OsV43jkR8ZER7RdQXFj0MuDzFL3Fq1EMEfgicE1EbDLO99F5wdOzMnPbzHx9eb/hXiEXilA8mtXHWU8V0WP5VylC7lzg5cBambl2Zm5Ufk0OWsb24z1uLTLzBope5lUoHoQyPHTkgLKJwxZ6MOhKkpruZxS9eLD0B39fRMRqwGvKl3+Xmf+TmfeNaLYRoygvYDspMw+k6CHckaIXNYBPRPGwi872mZk/zsz3ZeZMit7idwH3AlsAn1ve99Unwz29UyJitJ7P4WDeq2d4tOEFw2OV/7pteSeFHSkC+AGZeWGXHuVRvybjOe4AOK2cDw9fOITil6DrMvOX9ZQ0+Ay6kqRGK6/0Hx7b+t5Rru5/koio0mu3AUt7LEcOMxj20irHg7+G2F9T9DjeTvFzeNQr+zPzvsw8FRju/d2z6vFWsKtY+gvG3t0alA9eGH54w9we+xnt/Qyv69z2r8E5M3sNP6jyNRnrcVeE4XveVjkXz6W4/du25a3shgOvvbmjMOhKktrgYxQXWG0C/FdErDFa44g4GPhghf0+yNIwt32X/TwDeG+PY6zWa6flHQoeL1+uXrafFBGjXTuzqLN93TLzXmB2+fLDPe4s8WGK23w9xNJfRkZ6Y0RsMXJheR/i4bsmfLtj1fB9hDeKiA27bLc9T35IRy9jPe6KMHyXjWnLapiZi4Gzy5efBV5IcQ6N9lCMlZ5BV5LUeJn5G+BwilC6P3BVeZeD9YbbRMQ6EfG6iJhNcaP+tbru7Mn7fYjijgQAZ0TEC8t9TYqIfSmGTfTqjfu3iDg3Ig4cUcdGEfF5irG7CVxcrlobuDEiPhoR20fE5BHHOq5sdyGD42iKXsmZwDnD44cjYs1y/PFRZbvjM/PBHvt4DPhh+fCJ4ff7apbeReDizPxFR/t5FL3hAXyzfGACEbFqRLyO4vMc7eK48R53Rbi2nL+8/KVpWYbvqTscxM/PzLv7X1aLZKaTk5OTk1MrJoonW91FESCHp4Us7ZkdnuYDe4zYdnjd9BHLX8LSR8wmRYgafr2AYgxvUj5VuGO7E0cc84EudXyko/20EeseK/f/RMeym4BNxviZzC+3PWaM23X9PLq0exfFeNmkCL33jqj5bGDyKHW9g+KhFMNfq87P+gbgGV22fW3HMbP8XB8t/30rxfjVBOb3+bjHlOvPHGW/e41YvtcotWxQfo2zfD93lPt5StuObX7dUeer6v4/N+iTPbqSpNbIzO9SXLB1OMWfym+nuFJ9FYoAcS7Fn7Wfm5mXVtznL4Gdge9S3FJsVYqA9GWKPx9f3WPTzwFHUNxt4XqKHsjVgdsoepT3yOLpYcMepHggwInAryguhFqL4rZgv6Z4pO4Ls3z62KDIzC9TPJ74vyiC2poUof5i4KDMPCS7P0xi2I3AiyjGmj5Acbu2+RR/nn9RZt7R5ZjnAfuUx1hI8TW5leKxvjuw9JZmoxnzcfstM++hGN/8PxRf76dTPMZ4s1E2+59yfgfwwxVaYAtE+duBJEmSBlxEXExxsd0JmXnUstqv7Ay6kiRJDVCOR76+fLl1dnmEsZ7MoQuSJEkDLiLWBE6mGAJzviG3Gnt0JUmSBlREvJ/iyXobU4zxXgzMyszraiyrMezRlSRJGlzTKC5OWwJcDuxnyK3OHl1JkiS1kj26kiRJaiWDriRJklrJoCtJkqRWWmW8G75s0kEO7pXUWBcPfTvqrkGStGLZoytJkqRWGnePriSpOSLiFmBtYH7NpUjSWE0HHszMzce6oUFXklYOa0+ZMmW9bbbZZr26C5GksZg3bx6LFi0a17YGXUlaOczfZptt1pszZ07ddUjSmMyaNYu5c+fOH8+2jtGVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVpJXENX96gOlH/YDpR/2g7lIkaUIYdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVpAEThbRFxZUQsjIhHIuKqiDgiIibXXZ8kNZFBV5IGw1nA6cDmwDeBrwCrAScB34yIqLE2SWqkVeouQJJWdhFxIHAocAuwY2beUy5fFfgW8HrgzcCZNZUoSY1kj64k1e915fyzwyEXIDMfB44uX753wquSpIYz6EpS/TYu5zd3WTe8bGZETJuYciSpHRy6IEn1G+7F3bzLui06/j0DuHK0HUXEnB6rZoyjLklqNHt0Jal+55fzD0bEesMLI2IV4NiOdutOaFWS1HD26EpS/c4BDgFeAVwXEf8LPAK8FNgSuAHYCliyrB1l5qxuy8ue3pn9KliSmsAeXUmqWWYOAQcAHwLupLgDw9uA24HdgAVl07trKVCSGsoeXUkaAJn5BPDZcvqriJgCvBBYBFw78ZVJUnPZoytJg+1QYA3gW+XtxiRJFRl0JWkARMTaXZa9GDgeeAj41wkvSpIazqELkjQYLo6IRcA1wELgecArgUeB12Vmt3vsSpJGYdCVpMFwLvAmirsvTAH+DJwGHJ+Z82usS5Iay6ArSQMgMz8NfLruOiSpTRyjK0mSpFYy6EqSJKmVHLogSSuJ7Z61DnOO37/uMiRpwtijK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6ErSgIiI/SPiooi4PSIWRcTNEfHtiNi57tokqYkMupI0ACLiBOB8YCbwI+AkYC7wGuAXEXFIjeVJUiOtUncBkrSyi4iNgQ8BdwHPz8y7O9btDfwU+Ffg7HoqlKRmskdXkuq3GcX34192hlyAzJwNLASeXkdhktRkBl1Jqt8NwGPAjhGxQeeKiNgDWAv4cR2FSVKTOXRBkmqWmfdGxIeB/wCui4jvAguALYEDgIuBd9VXoSQ1k0FXkgZAZp4YEfOBM4B3dqy6EThz5JCGXiJiTo9VM5avQklqHocuSNIAiIh/As4FzqToyZ0KzAJuBr4REf9eX3WS1Ez26EpSzSJiL+AE4LzM/GDHqrkR8VrgeuDIiDglM28ebV+ZOavHMeZQ3LpMklYa9uhKUv1eVc5nj1yRmY8Av6L4fr3DRBYlSU1n0JWk+q1eznvdQmx4+WMTUIsktYZBV5Lqd1k5PywintW5IiJeAewKLAYun+jCJKnJHKMrSfU7l+I+uS8F5kXEecCdwDYUwxoCOCozF9RXoiQ1j0FXkmqWmUMR8UrgcOBNwGuBpwH3AhcAn8/Mi2osUZIayaArSQMgMx8HTiwnSVIfOEZXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kg+MkKSVxDV/eoDpR/2g7jIkNdT84/evu4Qxs0dXkiRJrWTQlSRJUisZdCVJktRKjtEdYdFrdqzc9s9vfKxy2w3/d43Kbdf9xe2V2z5xW/W2GptVNt2kctub3vHsym3PPPTkym1fvHpUbvvF+zev3Pb8561bua0kSU1lj64kDYCIeEtE5DKmJXXXKUlNYo+uJA2G3wDH9li3O7AP8MMJq0aSWsCgK0kDIDN/QxF2nyIirij/eepE1SNJbeDQBUkaYBGxHbAT8CfAm+BK0hgYdCVpsL2rnJ+emY7RlaQxcOiCJA2oiJgCHAIMAadV3GZOj1Uz+lWXJDWFPbqSNLgOBqYBP8zM22quRZIaxx5dSRpch5XzL1fdIDNndVte9vTO7EdRktQU9uhK0gCKiG2BXYDbgQtqLkeSGsmgK0mDyYvQJGk5OXRhhNtePVS57fV7nl657dCe1fd7/sPrV2475+HpldtqbGZNvbRy2wOm3le57RDVz4WhMfwueti0Gyu3PevtR1Ruu/7pVyy7kfoqItYADqW4CK36NxpJ0pPYoytJg+cgYF3gAi9Ck6TxM+hK0uAZvgjNJ6FJ0nIw6ErSAImIbYDd8CI0SVpujtGVpAGSmfOAqLsOSWoDe3QlSZLUSgZdSZIktZJDFyRpJbHds9ZhzvH7112GJE0Ye3QlSZLUSgZdSZIktZJBV5IkSa3kGN0Rpt6wWuW2k14xljsAVf+d4sCp91due8DUq8ZQQfV6h8ha99ukWse637GcC6vG5MptH69eLos38O5VkqT2s0dXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVpgETE7hHxnYi4IyIeLecXRcQr665NkprG++hK0oCIiI8BnwDuAc4H7gA2AHYA9gIuqK04SWogg64kDYCIOIgi5P4YeF1mLhyxftVaCpOkBnPogiTVLCImAScAjwB/OzLkAmTm4xNemCQ1nD26y2Esj4gdYmgMe67++0d799ukWlfcfsfyWN+x7Hezb9xaue0T1UvQ+O0CbA6cC9wXEfsD2wGLgV9l5hV1FidJTWXQlaT6vbic3wXMBbbvXBkRlwJvyMy/LGtHETGnx6oZy1WhJDWQQxckqX4blvN3A1OAlwJrUfTqXgjsAXy7ntIkqbns0ZWk+k0u50HRc3t1+fraiHgtcD2wZ0TsvKxhDJk5q9vysqd3Zr8KlqQmsEdXkup3Xzm/uSPkApCZiyh6dQF2nNCqJKnhDLqSVL8/lPP7e6wfDsJTVnwpktQeBl1Jqt+lFDe42CoiVuuyfrtyPn/CKpKkFjDoSlLNMvMe4JvAOsC/dK6LiJcBfwM8APxo4quTpObyYjRJGgwfBF4CfDQi9gB+BWwGvBZYArwzM++vrzxJah6DriQNgMy8OyJeAnyMItzuBCwEfgB8KjOvrLM+SWoig64kDYjMvJeiZ/eDddciSW1g0F0Ok4jKbff63Rsrt124ePXKbV+12bWV2zbJf89ZMXdRmnpDt+t8lt+0ve+s3Hb29tXv+79qTF52o9IW576nctutbv9l5baSJDWVF6NJkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiUfAbwchsjKbaecMK1y2zVnz63cdk5Lf1fZmv+ru4QxOf+IOZXbDjFUue2Vi6vXMONL91Zuu6T6biVJaqx2piRJapiImB8R2WO6s+76JKmJ7NGVpMHxAHBil+UPTXAdktQKBl1JGhz3Z+YxdRchSW3h0AVJkiS1kj26kjQ4Vo+IQ4BnAw8DvwUuzUyvH5SkcTDoStLg2Bj4+ohlt0TEWzPzZ3UUJElNZtCVpMHwVeAy4FpgIbAF8I/AYcAPI2LnzLx6WTuJiF73upvRr0IlqSkMupI0ADLz2BGLrgHeHREPAUcCxwCvnei6JKnJDLqSNNhOoQi6e1RpnJmzui0ve3pn9rEuSRp43nVBkgbb3eV8aq1VSFID2aO7HCYRldtu+qkbKrf9807jqUb9dvO/71y57SSqP7Z5LL9f/uNx/1i57frzrhhDDWqQ4RPx5lqrkKQGskdXkmoWEc+LiPW6LN8M+EL58uyJrUqSms8eXUmq30HAURExG7iF4q4LWwL7A2sAFwCfqa88SWomg64k1W828FxgB4qhClOB+4GfU9xX9+uZmbVVJ0kNZdCVpJqVD4PwgRCS1GeO0ZUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIredeFETb7xq2V2w4dUf1uP5fd9JzKbbfkqsptNUY7bl+56U/e9OnKbYeYMoa2Q5Xbrn+6TzuTJGm87NGVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSBlREHBoRWU7vqLseSWoag64kDaCI2BQ4GXio7lokqakMupI0YCIigK8CC4BTai5HkhrLRwCP8MTtf6rcdhJRue3QgtXGU4767Nb916rc9hmTqz/Wdyznwo6fel/lthtyeeW2apUjgH2Avcq5JGkc7NGVpAESEdsAxwMnZealddcjSU1mj64kDYiIWAX4OvBH4CPj3MecHqtmjLcuSWoqg64kDY5/AXYAdsvMRXUXI0lNZ9CVpAEQETtS9OJ+NjOvGO9+MnNWj/3PAWaOd7+S1ESO0ZWkmnUMWbgeOLrmciSpNQy6klS/NYGtgW2AxR0PiUjg42Wbr5TLTqyrSElqGocuSFL9HgVO77FuJsW43Z8DfwDGPaxBklY2Bl1Jqll54VnXR/xGxDEUQfeszDxtIuuSpKZz6IIkSZJayaArSZKkVnLownLY/cj3VG474+oFldsuGU8xK7EFb9+5ctszDz25ctshhsZQRfXfGde/dvEY9quVXWYeAxxTcxmS1Ej26EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJR8BvBzWOufKym19rO/YTJ62TuW2b/7gBZXbvnj1qNx2aAy/B77+xv0rt508e27ltpIkafzs0ZUkSVIrGXQlSZLUSgZdSRoAEXFCRPwkIm6LiEURcW9EXBURH4+I9euuT5KayKArSYPhA8BU4GLgJOAbwBPAMcBvI2LT+kqTpGbyYjRJGgxrZ+bikQsj4jjgI8A/A++Z8KokqcHs0ZWkAdAt5Ja+Vc63mqhaJKktDLqSNNheXc5/W2sVktRADl2QpAESER8C1gTWAV4E7EYRco+vuP2cHqtm9KVASWoQg64kDZYPARt1vP4R8JbM/EtN9UhSYxl0JWmAZObGABGxEbALRU/uVRHxqsxc5mP1MnNWt+VlT+/MftYqSYPOoKuB9PtPVP8r63nTfly57Vge6zvEUOW2S/5ucuW2UhWZeRdwXkTMBa4HvgZsV29VktQsXowmSQMsM28FrgOeFxEb1F2PJDWJQVeSBt8zy/mSWquQpIYx6EpSzSJiRkRs3GX5pPKBERsCl2fmfRNfnSQ1l2N0Jal+Lwc+HRGXAjcBCyjuvLAnsAVwJ/DO+sqTpGYy6EpS/X4MnArsCrwAmAY8THER2teBz2fmvbVVJ0kNZdCVpJpl5jXA4XXXIUlt4xhdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSl6Mpgm1yqabVGp35L4XVN7npDH8vjaJqNx2r9+9sXLbNW+/uXJbSZI0MezRlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkmoWEetHxDsi4ryIuDEiFkXEAxHx84h4e0T4vVqSxsEHRkhS/Q4CvgTcAcwG/ghsBLwOOA14RUQclJlZX4mS1DwGXUmq3/XAAcAPMnNoeGFEfAT4FfB6itD7nXrKk6RmMuhqQk0++4lK7Q5bZ37lfQ4xtOxGpVPuf07ltuscfE/ltksqt5SeKjN/2mP5nRFxCnAcsBcGXUkaE8d9SdJge7ycV/stUZL0VwZdSRpQEbEK8Pflyx/VWYskNZFDFyRpcB0PbAdckJkXVtkgIub0WDWjb1VJUkPYoytJAygijgCOBH4PHFpzOZLUSPboStKAiYjDgZOA64B9M/Peqttm5qwe+5wDzOxPhZLUDPboStIAiYj3A18ArgH2zsw7661IkprLoCtJAyIiPgx8DvgNRci9u96KJKnZDLqSNAAi4miKi8/mUAxXqH4jZ0lSV47RlaSaRcSbgX+lePbIZcARETGy2fzMPHOCS5OkRjPoSlL9Ni/nk4H392jzM+DMiShGktrCoKvl9sQ+XS/y7urYTU+t1G4ST+nNGrV1VV/+2v6V2z7rwcvHUIM0fpl5DHBMzWVIUus4RleSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSK/kIYC23406r9lhfgB1WH6rUbmgMv4P95/1bVm672Tdurdz2icotJUnSILJHV5IkSa1k0JUkSVIrGXQlaQBExBsi4uSIuCwiHoyIjIiz665LkprMMbqSNBg+BrwAeAi4HZhRbzmS1Hz26ErSYPgAsDWwNvAPNdciSa1gj64kDYDMnD3874iosxRJag17dCVJktRK9uhKUotExJweqxzzK2mlY4+uJEmSWskeXUlqkcyc1W152dM7c4LLkaRaGXTV1aLX7Fi57YtXn1u5bdVH+06i+sU4X/jN3pXbbnn7VZXbSpKkZnPogiRJklrJoCtJkqRWMuhKkiSplRyjK0kDICIOBA4sX25czneOiDPLf9+TmR+a4LIkqdEMupI0GF4IvHnEsi3KCeBWwKArSWPg0AVJGgCZeUxmxijT9LprlKSmMehKkiSplQy6kiRJaiWDriRJklrJi9FWIqtsuknltrsdc2XltkPkGNoOVWp32G37VN7n9NOqP0VNkiStPOzRlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVpQETEJhFxRkT8OSIejYj5EXFiRKxbd22S1EQ+AnglsnirjSq3PXbD71VuO4mxPIK32u9Wv/vKdpX3uP7sK8ZwfGkwRcSWwOXAhsD3gN8DOwLvA14eEbtm5oIaS5SkxrFHV5IGwxcpQu4RmXlgZh6VmfsAnwOeCxxXa3WS1EAGXUmqWURsAewHzAf+c8TqjwMPA4dGxNQJLk2SGs2gK0n126ecX5SZQ50rMnMh8AvgacBOE12YJDWZY3QlqX7PLefX91h/A0WP79bAT0bbUUTM6bFqxvhKk6TmskdXkuq3Tjl/oMf64eXTVnwpktQe9uhK0uAbvrVJLqthZs7quoOip3dmP4uSpEFnj64k1W+4x3adHuvXHtFOklSBQVeS6veHcr51j/VblfNeY3glSV0YdCWpfrPL+X4R8aTvyxGxFrArsAi4cqILk6QmM+hKUs0y8ybgImA6cPiI1ccCU4GvZebDE1yaJDWaF6OtRFa/c2Hltofdtlfltqdueknltnv89uBK7dY/3cf6aqXzHopHAH8+IvYF5gEvAfamGLLw0Rprk6RGskdXkgZA2av7IuBMioB7JLAl8Hlg58xcUF91ktRM9uhK0oDIzNuAt9ZdhyS1hT26kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplXxgxEpkyXXXV277552q7/dVzKrcdm1uqr5jSZKk5WCPriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRW8vZikrRymD5v3jxmzap+O0BJGgTz5s0DmD6ebQ26krRyWHPRokVL5s6de3XdhQyQGeX897VWMVj8TJ7Kz+SpJvozmQ48OJ4NDbqStHK4BiAz7dItRcQc8DPp5GfyVH4mT9Wkz8QxupIkSWqlcffoXjz07ehnIZIkSVI/2aMrSZKkVjLoSpIkqZUMupIkSWqlyMy6a5AkSZL6zh5dSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlaYBFxCYRcUZE/DkiHo2I+RFxYkSsu6L3ExG7RMQFEXFvRDwSEb+NiPdHxOTlf2fjt7yfSUSsHxHviIjzIuLGiFgUEQ9ExM8j4u0R8ZSfjRExPSJylOmc/r/T6vpxnpTb9Hp/d46yXVvPk7cs42ueEbFkxDYDe55ExBsi4uSIuCwiHizrOXuc+2rM9xMfGCFJAyoitgQuBzYEvgf8HtgR2Bv4A7BrZi5YEfuJiNcA3wEWA98E7gVeDTwXODczD+rDWxyzfnwmEfFu4EvAHcBs4I/ARsDrgHUo3vdB2fEDMiKmA7cAVwPf7bLbazLz3OV4a+PWx/NkPjANOLHL6ocy8zNdtmnzefJC4MAeq3cH9gF+kJmv6thmOoN7nvwGeAHwEHA7MAP4RmYeMsb9NOv7SWY6OTk5OQ3gBFwIJPDeEcv/o1x+yorYD7A2cDfwKPCijuVrUPyAS+BNTf1MKALKq4FJI5ZvTBF6E3j9iHXTy+Vn1n1erMDzZD4wfwzHbfV5soz9X1Hu54AGnSd7A1sBAexV1nn2iv5s6z5Pav/gnZycnJyeOgFblD8AbukSyNai6JV5GJja7/0Abyu3OavL/vYp1/2sqZ/JMo7xkfIYJ49YPpABpp+fyTiC7kp5ngDblfu/HZjchPOky3sYV9Bt4vcTx+hK0mDap5xflJlDnSsycyHwC+BpwE4rYD/D2/yoy/4uBR4BdomI1Zf1JvqsX5/JaB4v50/0WP/MiHhXRHyknD9/OY7VD/3+TFaPiEPK9/e+iNh7lDGUK+t58q5yfnpmLunRZtDOk35p3PcTg64kDabnlvPre6y/oZxvvQL203ObzHyCojdnFYrenYnUr8+kq4hYBfj78mW3H8oALwNOAY4r51dHxOyIePZ4jtkH/f5MNga+TvH+TgR+CtwQEXuO5dhtPU8iYgpwCDAEnDZK00E7T/qlcd9PDLqSNJjWKecP9Fg/vHzaCthPv47dbyu6ruMp/ix9QWZeOGLdI8AngFnAuuW0J8XFbHsBP4mIqeM87vLo52fyVWBfirA7Fdge+DLFn+N/GBEvWIHH7qcVWdfB5XY/zMzbuqwf1POkXxr3/cSgK0nNFOV8eW+dM5799OvY/TbuuiLiCOBIiivIDx25PjPvzsx/ycy5mXl/OV0K7Af8EngO8I7xl77CVP5MMvPYzPxpZt6VmY9k5jWZ+W6Ki4ymAMesqGNPsOWp67By/uVuKxt8nvTLwH0/MehK0mAa7uVYp8f6tUe06+d++nXsflshdUXE4cBJwHXA3pl5b9Vtyz+9Dv8Je4+xHLdPJuJrdUo5H/n+VrbzZFtgF4qL0C4Yy7YDcJ70S+O+nxh0JWkw/aGc9xpHuFU57zVWbnn203Obchzr5hQXa928jGP3W78+k7+KiPcDXwCuoQi5PR+MMIq/lPM6/iTd98+ki7vL+cj3t9KcJ6UqF6GNps7zpF8a9/3EoCtJg2l2Od8vRjypKyLWAnYFFgFXroD9/LScv7zL/vaguKr68sx8dFlvos/69ZkMb/Nh4HPAbyhC7t2jb9HT8BXmEx3ooM+fSQ87l/OR72+lOE/K7dagGNIyBJw+zrrqPE/6pXHfTwy6kjSAMvMm4CKKC4EOH7H6WIpeoa9l5sMAEbFqRMwon1o07v2UzgXuAd4UES8aXlj+sP9k+fJL435z49Svz6RcdzTFxWdzgH0z857Rjh0RL4mI1bos3wf4QPlyXI9TXR79+kwi4nkRsd7I/UfEZhQ93vDU99f686TDQRQXll3Q4yI0yn0N5HkyVm36fuIjgCVpQHV51OY84CUUTzi6Htgly0dtdjx69NbMnD7e/XRscyDFD6jFwDkUj+w8gPKRncDBWcMPkH58JhHxZuBMYAlwMt3HBs7PzDM7trkEeB5wCcUYTYDns/QeoUdn5iepQZ8+k2OAoyh67G4BFgJbAvtTPMHqAuC1mfnYiGMfSEvPkxH7uwzYjeJJaN8f5biXMLjnyYEsfaTxxsDfUPQuX1YuuyczP1S2nU5bvp+sqCdRODk5OTkt/wRsSnHbpzuAx4BbKS6cWm9Eu+kUVy3PX579jNhmV4qAcx/FnyN/R9ErNblf76+Oz4Ti7gG5jOmSEdu8HTif4ulhD1E8zvSPwDeB3Zt+nlDcAuu/Ke46cT/FgzP+AlxMcW/hWNnOk47125Trb1vWexrk86TCeT+/o21rvp/YoytJkqRWcoyuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWun/A+SBXjRFnyuIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)                               # view used for reshape the image\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">MNIST Clasification: Exercise</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 1:</h3>\n",
    "  <p>Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.</p>\n",
    "  <p>Build a network to classify the MNIST images with 3 hidden layers. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\02. MLP Muliclass classification (MNIST)\\4.3 Pytorch MNIST - Exercise.ipynb Cell 59'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=3'>4</a>\u001b[0m output_size  \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=5'>6</a>\u001b[0m \u001b[39m# Build a feed-forward network\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=7'>8</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mfc1\u001b[39m\u001b[39m'\u001b[39m,   nn\u001b[39m.\u001b[39mLinear(input_size, hidden_sizes[\u001b[39m0\u001b[39m])),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=8'>9</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mrelu1\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mReLU()),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=9'>10</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mfc2\u001b[39m\u001b[39m'\u001b[39m,   nn\u001b[39m.\u001b[39mLinear(hidden_sizes[\u001b[39m0\u001b[39m], hidden_sizes[\u001b[39m1\u001b[39m])),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=10'>11</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mrelu2\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mReLU()),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=11'>12</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mfc3\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mLinear(hidden_sizes[\u001b[39m1\u001b[39m], hidden_sizes[\u001b[39m2\u001b[39m])),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=12'>13</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mrelu3\u001b[39m\u001b[39m'\u001b[39m, nn\u001b[39m.\u001b[39mReLU()),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=13'>14</a>\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m,   nn\u001b[39m.\u001b[39mLinear(hidden_sizes[\u001b[39m2\u001b[39m], output_size)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=14'>15</a>\u001b[0m           \u001b[39m#('softmax', nn.Softmax(dim=1))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=15'>16</a>\u001b[0m           ]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000058?line=16'>17</a>\u001b[0m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [400, 200, 100]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('fc3', nn.Linear(hidden_sizes[1], hidden_sizes[2])),\n",
    "          ('relu3', nn.ReLU()),\n",
    "          ('output',   nn.Linear(hidden_sizes[2], output_size)),\n",
    "          #('softmax', nn.Softmax(dim=1))\n",
    "          ]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 2:</h3>\n",
    "  <p>Train your network implementing the Pytorch training loop and <strong style=\"color:#01ff84\">after each epoch, use the model for predicting the test (validation) MNIST data.</strong></p>\n",
    "  <p>Note: If your model does not fit with the final softmax layer, you can remove this layer.</p>\n",
    "  <p>Hint: <a href=\"https://discuss.pytorch.org/t/training-loop-checking-validation-accuracy/78399\">Training loop checking validation accuracy\n",
    "</a></p>\n",
    "  <p>Research about <code>model.train()</code>, <code>model.eval()</code> and <code>with torch.no_grad()</code> in Pytorch.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "\tIteration: 0\t Loss: 0.1008\n",
      "\tIteration: 20\t Loss: 2.1322\n",
      "\tIteration: 40\t Loss: 2.1232\n",
      "\tIteration: 60\t Loss: 2.1452\n",
      "\tIteration: 80\t Loss: 2.1189\n",
      "\tIteration: 100\t Loss: 2.1167\n",
      "\tIteration: 120\t Loss: 2.1191\n",
      "\tIteration: 140\t Loss: 2.1291\n",
      "\tIteration: 160\t Loss: 2.0995\n",
      "\tIteration: 180\t Loss: 2.1149\n",
      "\tIteration: 200\t Loss: 2.1223\n",
      "\tIteration: 220\t Loss: 2.1125\n",
      "\tIteration: 240\t Loss: 2.0977\n",
      "\tIteration: 260\t Loss: 2.0983\n",
      "\tIteration: 280\t Loss: 2.0988\n",
      "\tIteration: 300\t Loss: 2.1082\n",
      "\tIteration: 320\t Loss: 2.1051\n",
      "\tIteration: 340\t Loss: 2.1210\n",
      "\tIteration: 360\t Loss: 2.1019\n",
      "\tIteration: 380\t Loss: 2.0964\n",
      "\tIteration: 400\t Loss: 2.1259\n",
      "\tIteration: 420\t Loss: 2.1082\n",
      "\tIteration: 440\t Loss: 2.1001\n",
      "\tIteration: 460\t Loss: 2.1182\n",
      "\tIteration: 480\t Loss: 2.0946\n",
      "\tIteration: 500\t Loss: 2.0956\n",
      "\tIteration: 520\t Loss: 2.1023\n",
      "\tIteration: 540\t Loss: 2.1016\n",
      "\tIteration: 560\t Loss: 2.0984\n",
      "\tIteration: 580\t Loss: 2.1136\n",
      "\tIteration: 600\t Loss: 2.1117\n",
      "\tIteration: 620\t Loss: 2.0957\n",
      "\tIteration: 640\t Loss: 2.0957\n",
      "\tIteration: 660\t Loss: 2.0965\n",
      "\tIteration: 680\t Loss: 2.0833\n",
      "\tIteration: 700\t Loss: 2.1058\n",
      "\tIteration: 720\t Loss: 2.1006\n",
      "\tIteration: 740\t Loss: 2.1045\n",
      "\tIteration: 760\t Loss: 2.1033\n",
      "\tIteration: 780\t Loss: 2.1009\n",
      "\tIteration: 800\t Loss: 2.0935\n",
      "\tIteration: 820\t Loss: 2.1036\n",
      "\tIteration: 840\t Loss: 2.0859\n",
      "\tIteration: 860\t Loss: 2.0931\n",
      "\tIteration: 880\t Loss: 2.0923\n",
      "\tIteration: 900\t Loss: 2.1061\n",
      "\tIteration: 920\t Loss: 2.0744\n",
      "Epoch: 2/10\n",
      "\tIteration: 0\t Loss: 0.1043\n",
      "\tIteration: 20\t Loss: 2.0918\n",
      "\tIteration: 40\t Loss: 2.0928\n",
      "\tIteration: 60\t Loss: 2.0902\n",
      "\tIteration: 80\t Loss: 2.0925\n",
      "\tIteration: 100\t Loss: 2.0819\n",
      "\tIteration: 120\t Loss: 2.1008\n",
      "\tIteration: 140\t Loss: 2.0994\n",
      "\tIteration: 160\t Loss: 2.0884\n",
      "\tIteration: 180\t Loss: 2.0984\n",
      "\tIteration: 200\t Loss: 2.1111\n",
      "\tIteration: 220\t Loss: 2.0837\n",
      "\tIteration: 240\t Loss: 2.0786\n",
      "\tIteration: 260\t Loss: 2.0874\n",
      "\tIteration: 280\t Loss: 2.0930\n",
      "\tIteration: 300\t Loss: 2.0756\n",
      "\tIteration: 320\t Loss: 2.0821\n",
      "\tIteration: 340\t Loss: 2.0792\n",
      "\tIteration: 360\t Loss: 2.0805\n",
      "\tIteration: 380\t Loss: 2.0747\n",
      "\tIteration: 400\t Loss: 2.0792\n",
      "\tIteration: 420\t Loss: 2.0850\n",
      "\tIteration: 440\t Loss: 2.1066\n",
      "\tIteration: 460\t Loss: 2.0892\n",
      "\tIteration: 480\t Loss: 2.0899\n",
      "\tIteration: 500\t Loss: 2.0828\n",
      "\tIteration: 520\t Loss: 2.0820\n",
      "\tIteration: 540\t Loss: 2.0643\n",
      "\tIteration: 560\t Loss: 2.0929\n",
      "\tIteration: 580\t Loss: 2.0938\n",
      "\tIteration: 600\t Loss: 2.0630\n",
      "\tIteration: 620\t Loss: 2.0917\n",
      "\tIteration: 640\t Loss: 2.0907\n",
      "\tIteration: 660\t Loss: 2.0833\n",
      "\tIteration: 680\t Loss: 2.0773\n",
      "\tIteration: 700\t Loss: 2.0864\n",
      "\tIteration: 720\t Loss: 2.0657\n",
      "\tIteration: 740\t Loss: 2.0739\n",
      "\tIteration: 760\t Loss: 2.0827\n",
      "\tIteration: 780\t Loss: 2.0819\n",
      "\tIteration: 800\t Loss: 2.0521\n",
      "\tIteration: 820\t Loss: 2.0667\n",
      "\tIteration: 840\t Loss: 2.0853\n",
      "\tIteration: 860\t Loss: 2.0864\n",
      "\tIteration: 880\t Loss: 2.0723\n",
      "\tIteration: 900\t Loss: 2.0794\n",
      "\tIteration: 920\t Loss: 2.0775\n",
      "Epoch: 3/10\n",
      "\tIteration: 0\t Loss: 0.1058\n",
      "\tIteration: 20\t Loss: 2.0711\n",
      "\tIteration: 40\t Loss: 2.0734\n",
      "\tIteration: 60\t Loss: 2.0791\n",
      "\tIteration: 80\t Loss: 2.0711\n",
      "\tIteration: 100\t Loss: 2.0549\n",
      "\tIteration: 120\t Loss: 2.0844\n",
      "\tIteration: 140\t Loss: 2.0652\n",
      "\tIteration: 160\t Loss: 2.0734\n",
      "\tIteration: 180\t Loss: 2.0552\n",
      "\tIteration: 200\t Loss: 2.0579\n",
      "\tIteration: 220\t Loss: 2.0793\n",
      "\tIteration: 240\t Loss: 2.0522\n",
      "\tIteration: 260\t Loss: 2.0663\n",
      "\tIteration: 280\t Loss: 2.0520\n",
      "\tIteration: 300\t Loss: 2.0751\n",
      "\tIteration: 320\t Loss: 2.0646\n",
      "\tIteration: 340\t Loss: 2.0679\n",
      "\tIteration: 360\t Loss: 2.0606\n",
      "\tIteration: 380\t Loss: 2.0609\n",
      "\tIteration: 400\t Loss: 2.0539\n",
      "\tIteration: 420\t Loss: 2.0559\n",
      "\tIteration: 440\t Loss: 2.0597\n",
      "\tIteration: 460\t Loss: 2.0681\n",
      "\tIteration: 480\t Loss: 2.0525\n",
      "\tIteration: 500\t Loss: 2.0362\n",
      "\tIteration: 520\t Loss: 2.0626\n",
      "\tIteration: 540\t Loss: 2.0673\n",
      "\tIteration: 560\t Loss: 2.0627\n",
      "\tIteration: 580\t Loss: 2.0552\n",
      "\tIteration: 600\t Loss: 2.0503\n",
      "\tIteration: 620\t Loss: 2.0537\n",
      "\tIteration: 640\t Loss: 2.0721\n",
      "\tIteration: 660\t Loss: 2.0533\n",
      "\tIteration: 680\t Loss: 2.0445\n",
      "\tIteration: 700\t Loss: 2.0522\n",
      "\tIteration: 720\t Loss: 2.0715\n",
      "\tIteration: 740\t Loss: 2.0366\n",
      "\tIteration: 760\t Loss: 2.0596\n",
      "\tIteration: 780\t Loss: 2.0626\n",
      "\tIteration: 800\t Loss: 2.0515\n",
      "\tIteration: 820\t Loss: 2.0439\n",
      "\tIteration: 840\t Loss: 2.0471\n",
      "\tIteration: 860\t Loss: 2.0451\n",
      "\tIteration: 880\t Loss: 2.0575\n",
      "\tIteration: 900\t Loss: 2.0556\n",
      "\tIteration: 920\t Loss: 2.0446\n",
      "Epoch: 4/10\n",
      "\tIteration: 0\t Loss: 0.1019\n",
      "\tIteration: 20\t Loss: 2.0309\n",
      "\tIteration: 40\t Loss: 2.0379\n",
      "\tIteration: 60\t Loss: 2.0575\n",
      "\tIteration: 80\t Loss: 2.0494\n",
      "\tIteration: 100\t Loss: 2.0454\n",
      "\tIteration: 120\t Loss: 2.0301\n",
      "\tIteration: 140\t Loss: 2.0318\n",
      "\tIteration: 160\t Loss: 2.0393\n",
      "\tIteration: 180\t Loss: 2.0623\n",
      "\tIteration: 200\t Loss: 2.0378\n",
      "\tIteration: 220\t Loss: 2.0431\n",
      "\tIteration: 240\t Loss: 2.0408\n",
      "\tIteration: 260\t Loss: 2.0419\n",
      "\tIteration: 280\t Loss: 2.0638\n",
      "\tIteration: 300\t Loss: 2.0290\n",
      "\tIteration: 320\t Loss: 2.0584\n",
      "\tIteration: 340\t Loss: 2.0336\n",
      "\tIteration: 360\t Loss: 2.0295\n",
      "\tIteration: 380\t Loss: 2.0256\n",
      "\tIteration: 400\t Loss: 2.0176\n",
      "\tIteration: 420\t Loss: 2.0195\n",
      "\tIteration: 440\t Loss: 2.0273\n",
      "\tIteration: 460\t Loss: 2.0338\n",
      "\tIteration: 480\t Loss: 2.0371\n",
      "\tIteration: 500\t Loss: 2.0436\n",
      "\tIteration: 520\t Loss: 2.0485\n",
      "\tIteration: 540\t Loss: 2.0339\n",
      "\tIteration: 560\t Loss: 2.0264\n",
      "\tIteration: 580\t Loss: 2.0154\n",
      "\tIteration: 600\t Loss: 2.0275\n",
      "\tIteration: 620\t Loss: 2.0463\n",
      "\tIteration: 640\t Loss: 2.0353\n",
      "\tIteration: 660\t Loss: 2.0107\n",
      "\tIteration: 680\t Loss: 2.0208\n",
      "\tIteration: 700\t Loss: 2.0161\n",
      "\tIteration: 720\t Loss: 2.0138\n",
      "\tIteration: 740\t Loss: 2.0301\n",
      "\tIteration: 760\t Loss: 2.0117\n",
      "\tIteration: 780\t Loss: 2.0227\n",
      "\tIteration: 800\t Loss: 2.0388\n",
      "\tIteration: 820\t Loss: 2.0176\n",
      "\tIteration: 840\t Loss: 2.0137\n",
      "\tIteration: 860\t Loss: 2.0279\n",
      "\tIteration: 880\t Loss: 2.0295\n",
      "\tIteration: 900\t Loss: 2.0180\n",
      "\tIteration: 920\t Loss: 2.0012\n",
      "Epoch: 5/10\n",
      "\tIteration: 0\t Loss: 0.1005\n",
      "\tIteration: 20\t Loss: 2.0098\n",
      "\tIteration: 40\t Loss: 2.0148\n",
      "\tIteration: 60\t Loss: 2.0193\n",
      "\tIteration: 80\t Loss: 2.0098\n",
      "\tIteration: 100\t Loss: 2.0083\n",
      "\tIteration: 120\t Loss: 2.0387\n",
      "\tIteration: 140\t Loss: 1.9944\n",
      "\tIteration: 160\t Loss: 2.0287\n",
      "\tIteration: 180\t Loss: 2.0172\n",
      "\tIteration: 200\t Loss: 1.9936\n",
      "\tIteration: 220\t Loss: 2.0084\n",
      "\tIteration: 240\t Loss: 1.9890\n",
      "\tIteration: 260\t Loss: 2.0179\n",
      "\tIteration: 280\t Loss: 2.0235\n",
      "\tIteration: 300\t Loss: 2.0053\n",
      "\tIteration: 320\t Loss: 1.9950\n",
      "\tIteration: 340\t Loss: 1.9835\n",
      "\tIteration: 360\t Loss: 2.0021\n",
      "\tIteration: 380\t Loss: 2.0072\n",
      "\tIteration: 400\t Loss: 2.0133\n",
      "\tIteration: 420\t Loss: 2.0146\n",
      "\tIteration: 440\t Loss: 1.9986\n",
      "\tIteration: 460\t Loss: 2.0244\n",
      "\tIteration: 480\t Loss: 2.0087\n",
      "\tIteration: 500\t Loss: 2.0023\n",
      "\tIteration: 520\t Loss: 2.0016\n",
      "\tIteration: 540\t Loss: 2.0112\n",
      "\tIteration: 560\t Loss: 1.9862\n",
      "\tIteration: 580\t Loss: 1.9893\n",
      "\tIteration: 600\t Loss: 2.0081\n",
      "\tIteration: 620\t Loss: 1.9865\n",
      "\tIteration: 640\t Loss: 2.0062\n",
      "\tIteration: 660\t Loss: 1.9917\n",
      "\tIteration: 680\t Loss: 1.9964\n",
      "\tIteration: 700\t Loss: 2.0132\n",
      "\tIteration: 720\t Loss: 1.9889\n",
      "\tIteration: 740\t Loss: 2.0146\n",
      "\tIteration: 760\t Loss: 1.9730\n",
      "\tIteration: 780\t Loss: 1.9819\n",
      "\tIteration: 800\t Loss: 1.9716\n",
      "\tIteration: 820\t Loss: 2.0050\n",
      "\tIteration: 840\t Loss: 1.9942\n",
      "\tIteration: 860\t Loss: 1.9767\n",
      "\tIteration: 880\t Loss: 1.9827\n",
      "\tIteration: 900\t Loss: 2.0179\n",
      "\tIteration: 920\t Loss: 1.9841\n",
      "Epoch: 6/10\n",
      "\tIteration: 0\t Loss: 0.1027\n",
      "\tIteration: 20\t Loss: 1.9951\n",
      "\tIteration: 40\t Loss: 2.0020\n",
      "\tIteration: 60\t Loss: 1.9841\n",
      "\tIteration: 80\t Loss: 2.0011\n",
      "\tIteration: 100\t Loss: 1.9737\n",
      "\tIteration: 120\t Loss: 1.9961\n",
      "\tIteration: 140\t Loss: 1.9891\n",
      "\tIteration: 160\t Loss: 1.9716\n",
      "\tIteration: 180\t Loss: 1.9700\n",
      "\tIteration: 200\t Loss: 1.9815\n",
      "\tIteration: 220\t Loss: 1.9961\n",
      "\tIteration: 240\t Loss: 1.9728\n",
      "\tIteration: 260\t Loss: 1.9899\n",
      "\tIteration: 280\t Loss: 1.9770\n",
      "\tIteration: 300\t Loss: 1.9999\n",
      "\tIteration: 320\t Loss: 1.9783\n",
      "\tIteration: 340\t Loss: 1.9573\n",
      "\tIteration: 360\t Loss: 1.9690\n",
      "\tIteration: 380\t Loss: 1.9660\n",
      "\tIteration: 400\t Loss: 1.9550\n",
      "\tIteration: 420\t Loss: 1.9696\n",
      "\tIteration: 440\t Loss: 1.9768\n",
      "\tIteration: 460\t Loss: 1.9484\n",
      "\tIteration: 480\t Loss: 1.9880\n",
      "\tIteration: 500\t Loss: 1.9887\n",
      "\tIteration: 520\t Loss: 1.9683\n",
      "\tIteration: 540\t Loss: 1.9622\n",
      "\tIteration: 560\t Loss: 1.9674\n",
      "\tIteration: 580\t Loss: 1.9771\n",
      "\tIteration: 600\t Loss: 1.9836\n",
      "\tIteration: 620\t Loss: 1.9886\n",
      "\tIteration: 640\t Loss: 1.9504\n",
      "\tIteration: 660\t Loss: 1.9695\n",
      "\tIteration: 680\t Loss: 1.9812\n",
      "\tIteration: 700\t Loss: 1.9690\n",
      "\tIteration: 720\t Loss: 1.9499\n",
      "\tIteration: 740\t Loss: 1.9641\n",
      "\tIteration: 760\t Loss: 1.9656\n",
      "\tIteration: 780\t Loss: 1.9480\n",
      "\tIteration: 800\t Loss: 1.9654\n",
      "\tIteration: 820\t Loss: 1.9686\n",
      "\tIteration: 840\t Loss: 1.9810\n",
      "\tIteration: 860\t Loss: 1.9666\n",
      "\tIteration: 880\t Loss: 1.9757\n",
      "\tIteration: 900\t Loss: 1.9820\n",
      "\tIteration: 920\t Loss: 1.9819\n",
      "Epoch: 7/10\n",
      "\tIteration: 0\t Loss: 0.0973\n",
      "\tIteration: 20\t Loss: 1.9746\n",
      "\tIteration: 40\t Loss: 1.9569\n",
      "\tIteration: 60\t Loss: 1.9747\n",
      "\tIteration: 80\t Loss: 1.9970\n",
      "\tIteration: 100\t Loss: 1.9852\n",
      "\tIteration: 120\t Loss: 1.9705\n",
      "\tIteration: 140\t Loss: 1.9682\n",
      "\tIteration: 160\t Loss: 1.9619\n",
      "\tIteration: 180\t Loss: 1.9409\n",
      "\tIteration: 200\t Loss: 1.9501\n",
      "\tIteration: 220\t Loss: 1.9670\n",
      "\tIteration: 240\t Loss: 1.9482\n",
      "\tIteration: 260\t Loss: 1.9398\n",
      "\tIteration: 280\t Loss: 1.9580\n",
      "\tIteration: 300\t Loss: 1.9478\n",
      "\tIteration: 320\t Loss: 1.9432\n",
      "\tIteration: 340\t Loss: 1.9349\n",
      "\tIteration: 360\t Loss: 1.9647\n",
      "\tIteration: 380\t Loss: 1.9497\n",
      "\tIteration: 400\t Loss: 1.9591\n",
      "\tIteration: 420\t Loss: 1.9313\n",
      "\tIteration: 440\t Loss: 1.9434\n",
      "\tIteration: 460\t Loss: 1.9688\n",
      "\tIteration: 480\t Loss: 1.9529\n",
      "\tIteration: 500\t Loss: 1.9681\n",
      "\tIteration: 520\t Loss: 1.9474\n",
      "\tIteration: 540\t Loss: 1.9598\n",
      "\tIteration: 560\t Loss: 1.9493\n",
      "\tIteration: 580\t Loss: 1.9584\n",
      "\tIteration: 600\t Loss: 1.9437\n",
      "\tIteration: 620\t Loss: 1.9350\n",
      "\tIteration: 640\t Loss: 1.9490\n",
      "\tIteration: 660\t Loss: 1.9483\n",
      "\tIteration: 680\t Loss: 1.9409\n",
      "\tIteration: 700\t Loss: 1.9730\n",
      "\tIteration: 720\t Loss: 1.9542\n",
      "\tIteration: 740\t Loss: 1.9543\n",
      "\tIteration: 760\t Loss: 1.9383\n",
      "\tIteration: 780\t Loss: 1.9497\n",
      "\tIteration: 800\t Loss: 1.9619\n",
      "\tIteration: 820\t Loss: 1.9528\n",
      "\tIteration: 840\t Loss: 1.9122\n",
      "\tIteration: 860\t Loss: 1.9553\n",
      "\tIteration: 880\t Loss: 1.9407\n",
      "\tIteration: 900\t Loss: 1.9392\n",
      "\tIteration: 920\t Loss: 1.9418\n",
      "Epoch: 8/10\n",
      "\tIteration: 0\t Loss: 0.0970\n",
      "\tIteration: 20\t Loss: 1.9460\n",
      "\tIteration: 40\t Loss: 1.9369\n",
      "\tIteration: 60\t Loss: 1.9458\n",
      "\tIteration: 80\t Loss: 1.9581\n",
      "\tIteration: 100\t Loss: 1.9518\n",
      "\tIteration: 120\t Loss: 1.9698\n",
      "\tIteration: 140\t Loss: 1.9563\n",
      "\tIteration: 160\t Loss: 1.9268\n",
      "\tIteration: 180\t Loss: 1.9428\n",
      "\tIteration: 200\t Loss: 1.9312\n",
      "\tIteration: 220\t Loss: 1.9345\n",
      "\tIteration: 240\t Loss: 1.9555\n",
      "\tIteration: 260\t Loss: 1.9465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\02. MLP Muliclass classification (MNIST)\\4.3 Pytorch MNIST - Exercise.ipynb Cell 63'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=4'>5</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39miter\u001b[39m(trainloader)):       \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=9'>10</a>\u001b[0m     \u001b[39m# Flatten MNIST images into a 784 long vector\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=10'>11</a>\u001b[0m     images\u001b[39m.\u001b[39mresize_(images\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m784\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/02.%20MLP%20Muliclass%20classification%20%28MNIST%29/4.3%20Pytorch%20MNIST%20-%20Exercise.ipynb#ch0000065?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/datasets/mnist.py?line=141'>142</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/datasets/mnist.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/datasets/mnist.py?line=144'>145</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/datasets/mnist.py?line=146'>147</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/datasets/mnist.py?line=147'>148</a>\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torchvision\\transforms\\transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=261'>262</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=262'>263</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=263'>264</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=264'>265</a>\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=267'>268</a>\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=268'>269</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/transforms.py?line=269'>270</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/functional.py?line=360'>361</a>\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/functional.py?line=361'>362</a>\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/functional.py?line=362'>363</a>\u001b[0m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39;49mdiv_(std)\n\u001b[0;32m    <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/torchvision/transforms/functional.py?line=363'>364</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "print_every = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):       \n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   \n",
    "        loss = criterion(output, labels) \n",
    "        loss.backward()                  \n",
    "        optimizer.step()                 \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works and predicts well for the validation data\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 3:</h3>\n",
    "  <p>Write the code for adding <strong style=\"color:#01ff84\">Early Stopping with patience = 2</strong> to the training loop from scratch.</p>\n",
    "  <p><strong style=\"color:#01ff84\">Hint:</strong> Monitor the Validation loss every epoch, and if in 2 epochs, the validation loss does not improve, stop the training loop with <code>break</code>.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your training loop here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
