{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:34:28.765516Z","iopub.execute_input":"2022-06-04T10:34:28.765986Z","iopub.status.idle":"2022-06-04T10:34:31.012001Z","shell.execute_reply.started":"2022-06-04T10:34:28.765885Z","shell.execute_reply":"2022-06-04T10:34:31.011188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"# Define a transform to normalize the data (Preprocessing) and cast to tensor\n   \ntrain_transform = transforms.Compose([\n                                    transforms.Resize((150,150)),\n#                                     \n                                    transforms.RandomRotation(20),\n                                    transforms.RandomResizedCrop(size=124),\n                                    transforms.RandomHorizontalFlip(),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(\n                                        mean=[0.485, 0.456, 0.406],\n                                        std=[0.229, 0.224, 0.225]) \n                                    ])\n    \n    \ntest_transform = transforms.Compose([\n                                        transforms.Resize((150,150)),\n                                        transforms.CenterCrop(124),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            mean=[0.485, 0.456, 0.406],\n                                            std=[0.229, 0.224, 0.225])\n        \n                                    ])\n\n\n\nroot_dir = '../input/intel-image-classification/' \n# Load the training data\ntrainset = datasets.ImageFolder(root_dir+'/seg_train/seg_train', transform = train_transform)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle = True)\n\n\n# Load the test data\ntestset = datasets.ImageFolder(root_dir + '/seg_test/seg_test',transform=test_transform)\ntestloader = DataLoader(testset, batch_size=64, shuffle=False)\n\n\nprint(trainloader.dataset, '\\n')\nprint(testloader.dataset)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:34:37.081921Z","iopub.execute_input":"2022-06-04T10:34:37.082427Z","iopub.status.idle":"2022-06-04T10:34:47.587969Z","shell.execute_reply.started":"2022-06-04T10:34:37.082393Z","shell.execute_reply":"2022-06-04T10:34:47.58714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, labels = iter(trainloader).next()\nprint(f'image size: {images[0].shape}')\ntrainset.classes","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:34:56.690833Z","iopub.execute_input":"2022-06-04T10:34:56.691202Z","iopub.status.idle":"2022-06-04T10:34:57.194883Z","shell.execute_reply.started":"2022-06-04T10:34:56.691171Z","shell.execute_reply":"2022-06-04T10:34:57.194001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inv_normalize =  transforms.Normalize(\n    mean=-1*np.divide([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n    std=1/np.array([0.229, 0.224, 0.225])\n)\n\ndef class_plot(data , classes ,inv_normalize = None,n_figures = 12):\n    n_row = int(n_figures/4)\n    fig,axes = plt.subplots(figsize=(14, 10), nrows = n_row, ncols=4)\n    for ax in axes.flatten():\n        idx = np.random.randint(len(data))\n        image,label = data[idx]\n        label = int(label)\n        l = classes[label]\n        if(inv_normalize!=None):\n            image = inv_normalize(image)\n        image = image.numpy().transpose(1,2,0)\n        im = ax.imshow(image)\n        ax.set_title(l)\n        ax.axis('off')\n    plt.show()\nclass_plot(trainset,trainset.classes,inv_normalize);","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:35:08.387492Z","iopub.execute_input":"2022-06-04T10:35:08.387906Z","iopub.status.idle":"2022-06-04T10:35:09.375818Z","shell.execute_reply.started":"2022-06-04T10:35:08.38787Z","shell.execute_reply":"2022-06-04T10:35:09.374965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = models.resnext50_32x4d(pretrained = True)\n\n# freezing paramaters\n# n_parameters = 0\n# for param in model.parameters():\n#     param.requires_grad = False\n\ninputs = model.fc.in_features\noutputs = len(trainset.classes)\n# model.fc = nn.Linear(inputs, outputs)\nclf = nn.Sequential(\n              nn.Dropout(0.30), \n              nn.Linear(inputs, outputs)\n                  )\n\nmodel.fc = clf","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:35:19.879694Z","iopub.execute_input":"2022-06-04T10:35:19.880155Z","iopub.status.idle":"2022-06-04T10:35:31.72866Z","shell.execute_reply.started":"2022-06-04T10:35:19.880117Z","shell.execute_reply":"2022-06-04T10:35:31.727604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\nmodel(images[0].unsqueeze(0)).shape","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:35:36.81281Z","iopub.execute_input":"2022-06-04T10:35:36.813189Z","iopub.status.idle":"2022-06-04T10:35:36.996165Z","shell.execute_reply.started":"2022-06-04T10:35:36.813157Z","shell.execute_reply":"2022-06-04T10:35:36.995283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and validation","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-06-04T10:35:42.046679Z","iopub.execute_input":"2022-06-04T10:35:42.047009Z","iopub.status.idle":"2022-06-04T10:35:42.100187Z","shell.execute_reply.started":"2022-06-04T10:35:42.046982Z","shell.execute_reply":"2022-06-04T10:35:42.099305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nlearning_rate = 0.001\nepochs = 30\n\noptimizer = optim.Adam(model.parameters(), lr= learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_losses = []\ntest_losses = []\ntrain_accuracies = []\ntest_accuracies = []\nbenchmark_accuracy = 0.90\nfor epoch in range(epochs):\n    print(f'Epoch {epoch + 1}/{epochs}')\n    running_accuracy = 0\n    running_loss = 0\n    # training\n    for x_train_batch, y_train_batch in trainloader:\n        x_train_batch = x_train_batch.to(device)\n        y_train_batch = y_train_batch.to(device)\n\n        optimizer.zero_grad()\n\n        # forward pass\n        logits = model(x_train_batch)\n        train_preds = torch.argmax(logits.detach(), dim=1)\n\n        # loss\n        train_loss = criterion(logits, y_train_batch)\n        running_loss += train_loss.item()\n\n        # train accuracy\n        train_acc = (y_train_batch == train_preds).sum() / len(y_train_batch)\n        running_accuracy += train_acc.item()\n\n        # backward pass\n        \n        train_loss.backward()\n        \n        # update paramaters\n        \n        optimizer.step()\n\n    # mean loss (all batches losses divided by the total number of batches)\n    train_losses.append(running_loss / len(trainloader))\n    \n    # mean accuracies\n    train_accuracies.append(running_accuracy / len(trainloader))\n    \n    # print\n    print(f'Train loss: {train_losses[-1] :.4f}')\n\n    # validation\n    model.eval()\n    with torch.no_grad():\n        running_accuracy = 0\n        running_loss = 0\n\n        for x_test_batch, y_test_batch in testloader:\n            x_test_batch = x_test_batch.to(device)\n            y_test_batch = y_test_batch.to(device)\n            # logits\n            test_logits = model(\n                x_test_batch)\n\n            # predictions\n            test_preds = torch.argmax(test_logits, dim=1)\n            \n            # accuracy\n            test_acc = (y_test_batch == test_preds).sum() / len(y_test_batch)\n            running_accuracy += test_acc.item()\n\n            # loss\n            test_loss = criterion(test_logits, y_test_batch)\n            running_loss += test_loss.item()\n\n        # mean accuracy for each epoch\n        test_accuracies.append(running_accuracy / len(testloader))\n\n        # mean loss for each epoch\n        test_losses.append(running_accuracy / len(testloader))\n        # print\n        print(f'Test accuracy: {test_accuracies[-1]*100 :.2f}%')\n        print('='*100)\n        # saving best model\n        # is current mean score (mean per epoch) greater than or equal to the benchmark?\n        if test_accuracies[-1] > benchmark_accuracy:\n            # save model to cpu\n            torch.save(model.to('cpu').state_dict(), './model.pth')\n            model.to(device) # bring back to gpu\n\n            # update benckmark\n            benchmark_accuracy = test_accuracies[-1]\n\n    model.train()\n\n\n# Plots\nx_epochs = list(range(epochs))\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.plot(x_epochs, train_losses, marker='o', label='train')\nplt.plot(x_epochs, test_losses, marker='o', label='test')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x_epochs, train_accuracies, marker='o', label='train')\nplt.plot(x_epochs, test_accuracies, marker='o', label='test')\nplt.axhline(benchmark_accuracy, c='grey', ls='--',\n            label=f'Best_accuracy({benchmark_accuracy*100 :.2f}%)')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.savefig('./learning_curve.png', dpi = 200)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}