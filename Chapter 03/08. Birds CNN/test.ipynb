{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "# from data_handler import testloader\n",
    "# from model import ConvNet\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnext50_32x4d(pretrained = True)\n",
    "\n",
    "inputs = model.fc.in_features\n",
    "outputs = 6\n",
    "\n",
    "model.fc = nn.Linear(inputs, outputs) \n",
    "\n",
    "model.load_state_dict( torch.load('C:\\\\Users\\\\ritth\\\\code\\\\Strive\\\\CNN-Weekend-Challenge\\\\model.pth') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize predictions\n",
    "def view_classify(img, ps):\n",
    "    labels = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "\n",
    "    class_pred = labels[np.argmax(ps, axis=1)]\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(15,15), ncols=2)\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(6), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(6))\n",
    "    ax2.set_yticklabels(np.arange(6))\n",
    "    ax2.set_title(class_pred)\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "                                        transforms.Resize((150,150)),\n",
    "                                        # transforms.CenterCrop(124),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = 'E:\\datasets\\intel_images\\seg_pred\\\\'\n",
    "# for img_pth in os.listdir(root_dir):\n",
    "#     img = Image.open(root_dir + img_pth)\n",
    "#     img_transf = test_transform(img)\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         logit = model(img_transf.unsqueeze(0))\n",
    "#     ps = nn.functional.softmax(logit, dim=1)\n",
    "#     view_classify(img, ps)\n",
    "#     plt.show()\n",
    "#     time.sleep(3)\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10004.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ritth\\code\\Strive\\Strive-Exercises\\Chapter 03\\08. Birds CNN\\test.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/08.%20Birds%20CNN/test.ipynb#ch0000005?line=1'>2</a>\u001b[0m root_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mritth\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcode\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mData\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mInlet data\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39marchive\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mseg_pred\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mseg_pred\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/08.%20Birds%20CNN/test.ipynb#ch0000005?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m img_pth \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(root_dir):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/08.%20Birds%20CNN/test.ipynb#ch0000005?line=3'>4</a>\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img_pth)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/08.%20Birds%20CNN/test.ipynb#ch0000005?line=4'>5</a>\u001b[0m     img_transf \u001b[39m=\u001b[39m test_transform(img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ritth/code/Strive/Strive-Exercises/Chapter%2003/08.%20Birds%20CNN/test.ipynb#ch0000005?line=5'>6</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\ritth\\software\\anaconda\\envs\\deep\\lib\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/PIL/Image.py?line=2949'>2950</a>\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/PIL/Image.py?line=2951'>2952</a>\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/PIL/Image.py?line=2952'>2953</a>\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/PIL/Image.py?line=2953'>2954</a>\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ritth/software/anaconda/envs/deep/lib/site-packages/PIL/Image.py?line=2955'>2956</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10004.jpg'"
     ]
    }
   ],
   "source": [
    "# Test on self downloaded images\n",
    "root_dir = 'C:\\\\Users\\\\ritth\\\\code\\\\Data\\\\Inlet data\\\\archive\\\\seg_pred\\\\seg_pred'\n",
    "for img_pth in os.listdir(root_dir):\n",
    "    img = Image.open(img_pth)\n",
    "    img_transf = test_transform(img)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = model(img_transf.unsqueeze(0))\n",
    "    ps = nn.functional.softmax(logit, dim=1)\n",
    "    view_classify(img, ps)\n",
    "    plt.show()\n",
    "    time.sleep(3)\n",
    "    clear_output(wait=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
