{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches, Batch Normalization and Dropout\n",
    "\n",
    "In this workbook you can experiment what you learnt about how to make batches out of your data, how to perform batch normalization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from data/batches_norm_drop.csv, then take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350140</td>\n",
       "      <td>4.248592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950728</td>\n",
       "      <td>3.528855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.371517</td>\n",
       "      <td>3.149416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.268221</td>\n",
       "      <td>4.337209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.881996</td>\n",
       "      <td>1.515387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>-3.425455</td>\n",
       "      <td>3.349783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>-1.513002</td>\n",
       "      <td>2.789840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-1.070356</td>\n",
       "      <td>3.484981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>-2.970848</td>\n",
       "      <td>3.443924</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-2.575695</td>\n",
       "      <td>2.140739</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1  2\n",
       "0    0.350140  4.248592  0\n",
       "1    0.950728  3.528855  0\n",
       "2    1.371517  3.149416  0\n",
       "3    0.268221  4.337209  0\n",
       "4    1.881996  1.515387  0\n",
       "..        ...       ... ..\n",
       "745 -3.425455  3.349783  2\n",
       "746 -1.513002  2.789840  2\n",
       "747 -1.070356  3.484981  2\n",
       "748 -2.970848  3.443924  2\n",
       "749 -2.575695  2.140739  2\n",
       "\n",
       "[750 rows x 3 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/batches_norm_drop.csv', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop([2], axis=1))\n",
    "y = np.array(df[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (750, 2)\n",
      "[[ 0.35014034  4.24859167]\n",
      " [ 0.95072765  3.52885487]\n",
      " [ 1.37151689  3.1494165 ]\n",
      " ...\n",
      " [-1.07035566  3.48498144]\n",
      " [-2.97084772  3.44392411]\n",
      " [-2.57569525  2.14073863]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\",X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to code your own function to create batches. If needed rewatch the video we provided in Eduflow.\n",
    "\n",
    "**Extra challange:**    Are you able to split between train and test _**without**_ using sklearn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of rows in train: 600\n",
      "Total Number of rows in test: 150\n"
     ]
    }
   ],
   "source": [
    "n_train = math.floor(0.8 * X.shape[0])\n",
    "n_test = math.ceil((1 - 0.8) * X.shape[0])\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "print(\"Total Number of rows in train:\",X_train.shape[0])\n",
    "print(\"Total Number of rows in test:\",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 2), (150, 2), (600,), (150,))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 2) (75, 2) (675,) (75,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert numpy to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.astype(np.float32))\n",
    "X_test = torch.tensor(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.tensor(y_train.astype(np.float32))\n",
    "y_test = torch.tensor(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 15\n",
    "\n",
    "\n",
    "n_batches = X_train.shape[0] // batch_size  \n",
    "n_batches_test = X_test.shape[0] // batch_size\n",
    "#print(n_batches)\n",
    "\n",
    "\n",
    "indexes = np.random.permutation(X_train.shape[0])\n",
    "indexes_test = np.random.permutation(X_test.shape[0])\n",
    "#print(indexes)\n",
    "\n",
    "\n",
    "X_train = X_train[indexes]\n",
    "y_train = y_train[indexes]\n",
    "\n",
    "X_test = X_test[indexes_test]\n",
    "y_test = y_test[indexes_test]\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train[ :batch_size * n_batches ].reshape(n_batches, batch_size, X_train.shape[1])\n",
    "y_train = y_train[ :batch_size * n_batches ].reshape(n_batches, batch_size, 1)\n",
    "#print(X_train)\n",
    "\n",
    "X_test = X_test[ :batch_size * n_batches_test ].reshape(n_batches_test, batch_size, X_test.shape[1])\n",
    "y_test = y_test[ :batch_size * n_batches_test ].reshape(n_batches_test, batch_size, 1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to create your model! Remember to include the new tricks you learnt (batch normalization and dropout)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=400, bias=True)\n",
      "  (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Dropout(p=0.2, inplace=False)\n",
      "  (7): Tanh()\n",
      "  (8): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (9): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): Dropout(p=0.1, inplace=False)\n",
      "  (11): Tanh()\n",
      "  (12): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (13): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = X_train.shape[2]\n",
    "hidden_sizes = [400, 200, 100]\n",
    "output_size   = 1\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.BatchNorm1d(400),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.BatchNorm1d(200),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.BatchNorm1d(100),\n",
    "                      nn.Dropout(0.1),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[2], output_size),\n",
    "                      nn.ReLU()\n",
    "                      \n",
    "                      )                \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model and evaluate it. **Extra challenge**: try to figure out how you can tell if batch norm and dropout are effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()             \n",
    "trainloss = []\n",
    "testloss = []\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 1, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 1, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 1, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 1, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 2, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 2, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 2, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 2, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 2, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 3, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 3, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 3, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 3, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 3, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 4, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 4, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 4, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 4, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 4, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 5, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 5, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 5, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 5, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 5, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 6, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 6, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 6, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 6, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 6, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 7, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 7, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 7, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 7, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 7, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 8, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 8, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 8, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 8, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 8, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 9, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 9, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 9, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 9, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 9, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 10, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 10, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 10, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 10, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 10, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 11, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 11, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 11, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 11, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 11, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 12, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 12, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 12, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 12, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 12, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 13, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 13, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 13, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 13, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 13, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 14, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 14, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 14, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 14, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 14, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 15, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 15, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 15, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 15, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 15, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 16, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 16, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 16, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 16, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 16, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 17, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 17, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 17, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 17, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 17, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 18, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 18, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 18, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 18, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 18, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 19, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 19, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 19, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 19, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 19, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 20, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 20, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 20, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 20, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 20, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 21, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 21, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 21, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 21, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 21, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 22, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 22, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 22, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 22, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 22, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 23, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 23, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 23, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 23, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 23, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 24, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 24, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 24, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 24, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 24, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 25, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 25, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 25, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 25, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 25, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 26, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 26, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 26, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 26, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 26, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 27, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 27, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 27, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 27, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 27, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 28, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 28, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 28, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 28, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 28, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 29, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 29, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 29, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 29, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 29, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 30, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 30, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 30, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 30, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 30, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 31, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 31, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 31, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 31, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 31, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 32, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 32, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 32, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 32, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 32, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 33, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 33, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 33, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 33, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 33, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 34, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 34, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 34, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 34, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 34, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 35, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 35, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 35, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 35, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 35, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 36, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 36, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 36, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 36, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 36, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 37, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 37, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 37, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 37, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 37, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 38, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 38, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 38, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 38, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 38, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 39, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 39, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 39, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 39, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 39, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 40, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 40, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 40, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 40, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 40, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 41, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 41, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 41, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 41, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 41, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 42, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 42, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 42, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 42, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 42, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 43, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 43, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 43, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 43, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 43, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 44, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 44, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 44, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 44, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 44, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 45, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 45, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 45, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 45, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 45, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 46, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 46, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 46, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 46, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 46, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 47, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 47, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 47, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 47, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 47, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 48, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 48, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 48, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 48, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 48, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 49, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 49, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 49, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 49, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 49, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 50, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 50, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 50, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 50, train loss: -0.00, test loss: -0.00\n",
      "Epoch: 50, train loss: -0.00, test loss: -0.00\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for X_train_batches, y_train_batches in zip(X_train, y_train):\n",
    "            model.train()             \n",
    "            y_pred = model(X_train_batches)\n",
    "            loss = criterion(y_pred, y_train_batches)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "            trainloss.append(running_loss / X_train.shape[0])\n",
    "\n",
    "            \n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for X_test_batches, y_test_batches in zip(X_test, y_test):\n",
    "\n",
    "                test_pred = model(X_test_batches)\n",
    "                test_loss = criterion(test_pred, y_test_batches)\n",
    "                running_loss += test_loss.item()\n",
    "                testloss.append(running_loss / X_test.shape[0])\n",
    "               \n",
    "        \n",
    "\n",
    "                print(f'Epoch: {epoch + 1}, train loss: {loss.item():.2f}, test loss: {test_loss.item():.2f}')\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "# plt.figure(figsize = (10, 8))\n",
    "# plt.title(\"Loss for Train and Test\")\n",
    "# plt.plot(trainloss, marker = 'o', label = 'Train Loss')\n",
    "# plt.plot(testloss, marker = 'o', label = 'Test Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6a404c1b23560d548308d831c1aa8041fb180aef1b35cf4a28ead3655e6085d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
